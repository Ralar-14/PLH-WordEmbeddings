{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLH - Rubén Álvarez Aragonés i Pol Pérez Prades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.ca.examples import sentences \n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Requisites\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedder:\n",
    "    def __init__(self, corpus_path, corpus_size, load_model=False, model_path=None):\n",
    "        if not load_model:\n",
    "            self.corpus_path = corpus_path\n",
    "            self.corpus_size = int(corpus_size * 2**30) if corpus_size else None  # Convert GB to bytes\n",
    "            self.corpus = self.get_corpus(corpus_path)\n",
    "            self.fit()\n",
    "        else:\n",
    "            try:\n",
    "                self.load(model_path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"Model not found. Please check the path.\")\n",
    "                return\n",
    "\n",
    "    def get_corpus(self, corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            print(\"Reading corpus...\")\n",
    "            corpus = f.read(self.corpus_size) if self.corpus_size else f.read()\n",
    "            print(\"Preprocessing corpus...\")\n",
    "            corpus = self.preprocess(corpus)  # Preprocess the corpus and tokenize it\n",
    "        return corpus\n",
    "\n",
    "    def fit(self, window_size=15, vector_size=300, min_count=10, workers=8, epochs=10):\n",
    "        # Initialize the Word2Vec model with gensim\n",
    "        print(\"Initializing Word2Vec model...\")\n",
    "        self.model = word2vec.Word2Vec(sentences=[self.corpus], vector_size=vector_size, window=window_size, min_count=min_count, workers=workers, epochs=epochs)\n",
    "        print(\"Model training completed.\")\n",
    "\n",
    "    def save(self, model_path):\n",
    "        # Save the model\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        # Load the model\n",
    "        self.model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        # Lowercase the corpus\n",
    "        print(\"Lowercasing...\")\n",
    "        corpus = corpus.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        print(\"Removing special characters...\")\n",
    "        corpus = re.sub(r'[^a-záàéèíìóòúùñüç\\s]', ' ', corpus)\n",
    "        \n",
    "        # Tokenize the corpus\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus = nltk.word_tokenize(corpus)\n",
    "        \n",
    "        # Eliminate last token (probably incomplete word)\n",
    "        corpus = corpus[:-1]\n",
    "        \n",
    "        return corpus\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        # Get the embedding of a word\n",
    "        try:\n",
    "            return self.model.wv[word]\n",
    "        except KeyError:\n",
    "            print(f\"Word '{word}' not in vocabulary.\")\n",
    "            return None\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocabulary:\", list(self.model.wv.index_to_key))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model amb 100MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.save('models/word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.print_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.model.wv.most_similar(\"negre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model amb 500MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500.model.wv.most_similar(\"inshalla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model amb 1GB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_1024 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model amb totes les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_full_data = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importació_data import read_all_ts_data, reformat_data, create_corpus, preprocess, flattened_corpus_count, stopwords_cat\n",
    "from importació_data import pair_list_to_x_y\n",
    "from model_bàsic import build_and_compile_model_better\n",
    "import tensorflow as tf\n",
    "from model_bàsic import compute_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim les stopwords del català i la funció del preprocessament del text, que les tindrà en compte a la hora de tokenitzar i natejar el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpw_cat = stopwords_cat()\n",
    "prepro = lambda x: preprocess(x, stpw_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per altra banda, llegim les dades i definim les variables més importants per a la creació dels models de similitud de text semàntic.\n",
    "- Llegim totes les dades de text similarity dividint-les en train, test i val. \n",
    "- Reformatejem les dades per a que siguin l'estructura List[Tuple[str, str, float]]. \n",
    "- Definim el corpus i el diccionari amb totes les paraules.\n",
    "- Creem un diccionari de python amb tots els indexs com a claus i amb la repetició de les paraules com a valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = read_all_ts_data()\n",
    "train, test, val = reformat_data(train, test, val)\n",
    "corpus, dictionary = create_corpus(train, test, val, preprocess=prepro)\n",
    "flat_corpus = flattened_corpus_count(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compartació amb diferents models de Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onehot import map_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un embedding OneHot té tamany igual a la llargada del diccionari. En la importació de dades ja hem eliminat les stopwords per reduïr la dimensió, però ara també eliminarem del embedding aquelles paraules que es repeteixen masses poques vegades o massa sovint. \n",
    "\n",
    "Per aconseguir això creem una llista que conté els indexs de les paraules que sí que utilitzarem i la passem com a argument a la funció *map_one_hot()*, per reduïr la dimensió del embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate from the dictionary the words that are repeated very few times or too many times\n",
    "keys_preprocess = [index  for index in dictionary if flat_corpus[index] > 10 and flat_corpus[index] < 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim les paraules a vectors OneHot amb la funció *map_one_hot()*. Aquesta funció crea un vector de zeros de la mida del diccionari i posa un 1 a la posició de la paraula en el diccionari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_one_hot_train = map_one_hot(train, dictionary, keys_preprocess)\n",
    "mapped_one_hot_test = map_one_hot(test, dictionary, keys_preprocess)\n",
    "mapped_one_hot_val = map_one_hot(val, dictionary, keys_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem el X i Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh, y_train_oh = pair_list_to_x_y(mapped_one_hot_train)\n",
    "x_val_oh, y_val_oh = pair_list_to_x_y(mapped_one_hot_val)\n",
    "x_test_oh, y_test_oh = pair_list_to_x_y(mapped_one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_oh: int = 64\n",
    "num_epochs_oh: int = 64\n",
    "\n",
    "train_dataset_oh = tf.data.Dataset.from_tensor_slices((x_train_oh, y_train_oh))\n",
    "train_dataset_oh = train_dataset_oh.shuffle(buffer_size=len(x_train_oh)).batch(batch_size_oh)\n",
    "\n",
    "val_dataset_oh = tf.data.Dataset.from_tensor_slices((x_val_oh, y_val_oh))\n",
    "val_dataset_oh = val_dataset_oh.batch(batch_size_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_oh = len(keys_preprocess)\n",
    "model_oh = build_and_compile_model_better(embedding_size = embedding_size_oh)\n",
    "tf.keras.utils.plot_model(model_oh, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_oh.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oh.fit(train_dataset_oh, epochs=num_epochs_oh, validation_data=val_dataset_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_oh, x_train_oh, y_train_oh)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_oh, x_val_oh, y_val_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_oh, x_test_oh, y_test_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podem observar a la hora de fer l'entrenament, el model fa molt *overfit*, així que no és capaç de generalitzar bé. Això és degut a que el model OneHot no té en compte la semàntica de les paraules, ja que cada paraula és tractada com a un element únic i no es té en compte el context en el que apareixen les paraules. Una altra raó per la que el model OneHot no és bó és perquè la mida dels vectors és molt gran, ja que la mida dels vectors és igual a la mida del diccionari, i això fa que el model sigui molt ineficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec preentrenats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_tf_idf import map_pairs_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegim el model de Word2Vec preentrenat en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_FILE = \"C:/Users/Pol/Downloads/cc.ca.300.bin.gz\"\n",
    "\n",
    "\n",
    "USE_MMAP = False\n",
    "if USE_MMAP:\n",
    "    from gensim.models.fasttext import FastTextKeyedVectors\n",
    "    MMAP_PATH = 'cc.ca.300.bin'\n",
    "    # wv_model.save(MMAP_PATH)\n",
    "    wv_model = FastTextKeyedVectors.load(MMAP_PATH, mmap='r')\n",
    "else:\n",
    "    from gensim.models import fasttext\n",
    "    wv_model = fasttext.load_facebook_vectors(WORD_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Mitjana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim a vectors les frases amb la funció *map_w2v()*. Aquesta funció obté l'embedding de Word2Vec de cada paraula de la frase i en fa la mitjana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_mean_train = map_pairs_w2v(train, wv_model, dictionary=dictionary, preprocess=prepro)\n",
    "mapped_w2v_mean_test = map_pairs_w2v(test, wv_model, dictionary=dictionary, preprocess=prepro)\n",
    "mapped_w2v_mean_val = map_pairs_w2v(val, wv_model, dictionary=dictionary, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem les dades en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_mean, y_train_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_train)\n",
    "x_val_w2v_mean, y_val_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_val)\n",
    "x_test_w2v_mean, y_test_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_mean: int = 64\n",
    "num_epochs_w2v_mean: int = 64\n",
    "\n",
    "train_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_train_w2v_mean, y_train_w2v_mean))\n",
    "train_dataset_w2v_mean = train_dataset_w2v_mean.shuffle(buffer_size=len(x_train_w2v_mean)).batch(batch_size_w2v_mean)\n",
    "\n",
    "val_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_val_w2v_mean, y_val_w2v_mean))\n",
    "val_dataset_w2v_mean = val_dataset_w2v_mean.batch(batch_size_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_w2v_mean = 300\n",
    "model_w2v_mean = build_and_compile_model_better(embedding_size = embedding_size_w2v_mean)\n",
    "tf.keras.utils.plot_model(model_w2v_mean, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_mean.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_mean.fit(train_dataset_w2v_mean, epochs=num_epochs_w2v_mean, validation_data=val_dataset_w2v_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_mean, x_train_w2v_mean, y_train_w2v_mean)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_mean, x_val_w2v_mean, y_val_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_mean, x_test_w2v_mean, y_test_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de vectors preentrenats amb mitjana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem observar com amb el model Word2vec i la mitjana el model segueix fent sobreajust però funciona molt millor que el OneHot. Això és degut a que el model Word2Vec té en compte la semàntica de les paraules, ja que les paraules que tenen un significat similar tenen un embedding similar. Això fa que el model sigui capaç de generalitzar millor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim el model de Tf-idf amb el corpus definit al inici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "modelo_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fent servir la funció *map_w2v_tfidf()* s'obté l'embedding de Word2Vec de cada paraula de la frase i s'aplica Tf-idf per obtenir un vector de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_tfidf_train = map_pairs_w2v(train, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_test = map_pairs_w2v(test, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_val = map_pairs_w2v(val, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem les dades en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_tfidf, y_train_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_train)\n",
    "x_val_w2v_tfidf, y_val_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_val)\n",
    "x_test_w2v_tfidf, y_test_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_tfidf: int = 64\n",
    "num_epochs_w2v_tfidf: int = 64\n",
    "\n",
    "train_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_train_w2v_tfidf, y_train_w2v_tfidf))\n",
    "train_dataset_w2v_tfidf = train_dataset_w2v_tfidf.shuffle(buffer_size=len(x_train_w2v_tfidf)).batch(batch_size_w2v_tfidf)\n",
    "\n",
    "val_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_val_w2v_tfidf, y_val_w2v_tfidf))\n",
    "val_dataset_w2v_tfidf = val_dataset_w2v_tfidf.batch(batch_size_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_w2v_tfidf = 300\n",
    "model_w2v_tfidf = build_and_compile_model_better(embedding_size = embedding_size_w2v_tfidf)\n",
    "tf.keras.utils.plot_model(model_w2v_tfidf, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_tfidf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_tfidf.fit(train_dataset_w2v_tfidf, epochs=num_epochs_w2v_tfidf, validation_data=val_dataset_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_tfidf, x_train_w2v_tfidf, y_train_w2v_tfidf)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_tfidf, x_val_w2v_tfidf, y_val_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_tfidf, x_test_w2v_tfidf, y_test_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions vectors preentrenats amb Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem observar com, fent us de Tf-idf, el model millora una mica més. Això és degut a que Tf-idf té en compte la freqüència de les paraules en el corpus, i això fa que les paraules que apareixen molt sovint tinguin un pes més baix. Això fa que el model sigui capaç de generalitzar millor. Tot i així el model segueix sobreajustant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importem tot allò necessari per a fer servir el model de SpaCy en català, inclòs el model preentrenat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_embed import map_spacy_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model de SpaCy en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_spacy()*. Aquesta funció utilitza el model de SpaCy per a convertir les frases a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_spacy_train = map_spacy_embed(train, spacy_model)\n",
    "mapped_spacy_test = map_spacy_embed(test, spacy_model)\n",
    "mapped_spacy_val = map_spacy_embed(val, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividim les particions en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sp, y_train_sp = pair_list_to_x_y(mapped_spacy_train)\n",
    "x_val_sp, y_val_sp = pair_list_to_x_y(mapped_spacy_val)\n",
    "x_test_sp, y_test_sp = pair_list_to_x_y(mapped_spacy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_sp: int = 64\n",
    "num_epochs_sp: int = 64\n",
    "\n",
    "train_dataset_sp = tf.data.Dataset.from_tensor_slices((x_train_sp, y_train_sp))\n",
    "train_dataset_sp = train_dataset_sp.shuffle(buffer_size=len(x_train_sp)).batch(batch_size_sp)\n",
    "\n",
    "val_dataset_sp = tf.data.Dataset.from_tensor_slices((x_val_sp, y_val_sp))\n",
    "val_dataset_sp = val_dataset_sp.batch(batch_size_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_sp = 300\n",
    "model_sp = build_and_compile_model_better(embedding_size = embedding_size_sp)\n",
    "tf.keras.utils.plot_model(model_sp, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_sp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp.fit(train_dataset_sp, epochs=num_epochs_sp, validation_data=val_dataset_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb al partició de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_train_sp, y_train_sp)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_sp, x_val_sp, y_val_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_test_sp, y_test_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilitzant el model de SpaCy, el model segueix fent sobreajust, i funciona de manera molt similar al Word2Vec, però una mica pitjor. Això és degut a que el model de SpaCy també té en compte la semàntica de les paraules, ja que les paraules que tenen un significat similar tenen un embedding similar. Això fa que el model sigui capaç de generalitzar millor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roberta_base import map_roberta_embed\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download ca_core_news_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model de RoBERTa preentrenat en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model = spacy.load(\"ca_core_news_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_roberta()*. Aquesta funció utilitza el model de RoBERTa per a convertir les frases a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_roberta_train = map_roberta_embed(train, roberta_model)\n",
    "mapped_roberta_test = map_roberta_embed(test, roberta_model)\n",
    "mapped_roberta_val = map_roberta_embed(val, roberta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividim les particions en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_roberta, y_train_roberta = pair_list_to_x_y(mapped_roberta_train)\n",
    "x_val_roberta, y_val_roberta = pair_list_to_x_y(mapped_roberta_val)\n",
    "x_test_roberta, y_test_roberta = pair_list_to_x_y(mapped_roberta_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_roberta: int = 64\n",
    "num_epochs_roberta: int = 64\n",
    "\n",
    "train_dataset_roberta = tf.data.Dataset.from_tensor_slices((x_train_roberta, y_train_roberta))\n",
    "train_dataset_roberta = train_dataset_roberta.shuffle(buffer_size=len(x_train_roberta)).batch(batch_size_roberta)\n",
    "\n",
    "val_dataset_roberta = tf.data.Dataset.from_tensor_slices((x_val_roberta, y_val_roberta))\n",
    "val_dataset_roberta = val_dataset_roberta.batch(batch_size_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_roberta = 768\n",
    "model_roberta = build_and_compile_model_better(embedding_size = embedding_size_roberta)\n",
    "tf.keras.utils.plot_model(model_roberta, show_shapes=True, show_layer_activations=True)\n",
    "print(model_roberta.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta.fit(train_dataset_roberta, epochs=num_epochs_roberta, validation_data=val_dataset_roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el model amb la partició de validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_roberta, x_train_roberta, y_train_roberta)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_roberta, x_val_roberta, y_val_roberta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provar el model amb la partició de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_roberta, x_test_roberta, y_test_roberta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem com el model de RoBERTa és un dels que millor funciona, ja que és el model més avançat i el que millor semàntica té. Això fa que el model sigui capaç de generalitzar millor.\n",
    "Per altra banda, els seus vectors són més grans així que és més pesat d'executar que els altres models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instal·lem les llibreries necessàries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tf-keras\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem els imports de les funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roberta_fine_tuned import prepare_roberta_ft, compute_pearson_roberta_ft, x_y_split_roberta_ft\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el model de RoBERTa fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta_ft = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer_roberta_ft = AutoTokenizer.from_pretrained(model_roberta_ft)\n",
    "pipe_roberta_ft = pipeline('text-classification', model=model_roberta_ft, tokenizer=tokenizer_roberta_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_roberta_ft, y_train_roberta_ft = x_y_split_roberta_ft(train)\n",
    "X_val_roberta_ft, y_val_roberta_ft = x_y_split_roberta_ft(val)\n",
    "X_test_roberta_ft, y_test_roberta_ft = x_y_split_roberta_ft(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_roberta_ft + X_val_roberta_ft + X_test_roberta_ft\n",
    "y = y_train_roberta_ft + y_val_roberta_ft + y_test_roberta_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenim els resultats del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe_roberta_ft(prepare_roberta_ft(X, tokenizer_roberta_ft), add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (all data): {compute_pearson_roberta_ft(predictions, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson_roberta_ft(predictions[-len(y_test_roberta_ft):], y_test_roberta_ft)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions Roberta fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sens dubte el model de RoBERTa fine-tuned és el que millor funciona, ja que és el model més avançat i el que millor semàntica té. Això fa que el model sigui capaç de generalitzar millor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models amb embeddings entrenables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAP_EMBEDDINGS: bool = True\n",
    "USE_PRETRAINED: bool = True\n",
    "MAX_LEN: int = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'map_w2v_trainable' from 'trainable_embed_model' (c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\trainable_embed_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrainable_embed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_w2v_trainable, model_2, pair_list_to_x_y\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'map_w2v_trainable' from 'trainable_embed_model' (c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\trainable_embed_model.py)"
     ]
    }
   ],
   "source": [
    "from trainable_embed_model import map_w2v_trainable, model_2, pair_list_to_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def map_word_embeddings(\n",
    "        sentence: str,\n",
    "        sequence_len: int,\n",
    "        fixed_dictionary: Optional[Dictionary] = None,\n",
    "        wv_model: Optional[tf.keras.layers.Embedding] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map to word-embedding indices\n",
    "    :param sentence:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence_preproc = simple_preprocess(sentence)[:sequence_len]\n",
    "    _vectors = np.zeros(sequence_len, dtype=np.int32)\n",
    "    index = 0\n",
    "    for word in sentence_preproc:\n",
    "        if fixed_dictionary is not None:\n",
    "            if word in fixed_dictionary.token2id:\n",
    "                # Sumo 1 porque el valor 0 está reservado a padding\n",
    "                _vectors[index] = fixed_dictionary.token2id[word] + 1\n",
    "                index += 1\n",
    "        else:\n",
    "            if word in wv_model.key_to_index:\n",
    "                _vectors[index] = wv_model.key_to_index[word] + 1\n",
    "                index += 1\n",
    "    return _vectors\n",
    "\n",
    "def map_w2v_trainable(\n",
    "        wv_model: tf.keras.layers.Embedding,\n",
    "    sentence_pairs: List[Tuple[str, str, float]],\n",
    "    sequence_len: int,\n",
    "    fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    '''\n",
    "    Mapea los pares de oraciones a pares de vectores\n",
    "    \n",
    "    Parameters:\n",
    "    - wv_model: Modelo de embeddings entrenable\n",
    "    - sentence_pairs: Lista de pares de oraciones\n",
    "    - sequence_len: Longitud de las secuencias\n",
    "    - fixed_dictionary: Diccionario fijo de palabras\n",
    "    \n",
    "    Returns:\n",
    "    - pares_vectores: Lista de pares de vectores\n",
    "    '''\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        vector1 = map_word_embeddings(sentence_1, sequence_len, fixed_dictionary, wv_model)\n",
    "        vector2 = map_word_embeddings(sentence_2, sequence_len, fixed_dictionary, wv_model)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores\n",
    "\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    '''\n",
    "    Convierte una lista de pares de vectores a un par de arrays de vectores y un array de etiquetas\n",
    "    \n",
    "    Parameters:\n",
    "    - pair_list: Lista de pares de vectores\n",
    "    \n",
    "    Returns:\n",
    "    - x: Par de arrays de vectores\n",
    "    - y: Array de etiquetas\n",
    "    '''\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.row_stack(_x_1), np.row_stack(_x_2)), np.array(_y)\n",
    "\n",
    "class MyLayer_mask(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.not_equal(x, 0)\n",
    "    \n",
    "class MyLayer_exp(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.exp(x)\n",
    "    \n",
    "class MyLayer_cast(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.cast(x, tf.float32)\n",
    "    \n",
    "class MyLayer_reduce_sum(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.reduce_sum(x, axis=1, keepdims=True)\n",
    "def model_2(\n",
    "    input_length: int,\n",
    "    dictionary_size: int = 1000,\n",
    "    embedding_size: int = 16,\n",
    "    learning_rate: float = 1e-3,\n",
    "    pretrained_weights: Optional[np.ndarray] = None,\n",
    "    trainable: bool = False,\n",
    "    use_cosine: bool = False,\n",
    ") -> tf.keras.Model:\n",
    "    # Inputs\n",
    "    input_1 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "    input_2 = tf.keras.Input((input_length,), dtype=tf.int32)\n",
    "\n",
    "    # Embedding Layer\n",
    "    if pretrained_weights is None:\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size, embedding_size, input_length=input_length, mask_zero=True\n",
    "        )\n",
    "    else:\n",
    "        dictionary_size = pretrained_weights.shape[0]\n",
    "        embedding_size = pretrained_weights.shape[1]\n",
    "        initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            dictionary_size,\n",
    "            embedding_size,\n",
    "            input_length=input_length,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=initializer,\n",
    "            trainable=trainable,\n",
    "        )\n",
    "\n",
    "    # Embed the inputs\n",
    "    embedded_1 = embedding(input_1)\n",
    "    embedded_2 = embedding(input_2)\n",
    "    # Pass through the embedding layer\n",
    "    _input_mask_1, _input_mask_2 = MyLayer_mask()(input_1), MyLayer_mask()(input_2)\n",
    "\n",
    "    # Attention Mechanism\n",
    "    attention_mlp = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='tanh'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    # Apply attention to each embedding\n",
    "    attention_weights_1 = attention_mlp(embedded_1)  \n",
    "    attention_weights_2 = attention_mlp(embedded_2) \n",
    "    # Mask the attention weights\n",
    "    attention_weights_1 = MyLayer_exp()(attention_weights_1) * MyLayer_cast()(_input_mask_1[:, :, None])\n",
    "    attention_weights_2 = MyLayer_exp()(attention_weights_2) * MyLayer_cast()(_input_mask_2[:, :, None])\n",
    "    # Normalize attention weights\n",
    "    attention_weights_1 = attention_weights_1 / MyLayer_reduce_sum()(attention_weights_1)\n",
    "    attention_weights_2 = attention_weights_2 / MyLayer_reduce_sum()(attention_weights_2)\n",
    "    # Compute context vectors\n",
    "    projected_1 = MyLayer_reduce_sum()(embedded_1 * attention_weights_1) \n",
    "    projected_2 = MyLayer_reduce_sum()(embedded_2 * attention_weights_2) \n",
    "    \n",
    "    if use_cosine:\n",
    "        # Compute the cosine distance using a Lambda layer\n",
    "        def cosine_distance(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return 2.5 * (1.0 + tf.reduce_sum(x1_normalized * x2_normalized, axis=1))\n",
    "        output = tf.keras.layers.Lambda(cosine_distance)([projected_1, projected_2])\n",
    "    else:\n",
    "         # Compute the cosine distance using a Lambda layer\n",
    "        def normalized_product(x):\n",
    "            x1, x2 = x\n",
    "            x1_normalized = tf.keras.backend.l2_normalize(x1, axis=1)\n",
    "            x2_normalized = tf.keras.backend.l2_normalize(x2, axis=1)\n",
    "            return x1_normalized * x2_normalized\n",
    "    \n",
    "        output = tf.keras.layers.Lambda(normalized_product)([projected_1, projected_2])\n",
    "        output = tf.keras.layers.Dropout(0.1)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            activation=\"relu\",\n",
    "        )(output)\n",
    "        output = tf.keras.layers.Dropout(0.2)(output)\n",
    "        output = tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "        )(output)\n",
    "        \n",
    "        output = tf.keras.layers.Lambda(lambda x: x * 5)(output)\n",
    "    # Model Definition\n",
    "    model = tf.keras.Model(inputs=(input_1, input_2), outputs=output)\n",
    "    model.compile(\n",
    "        loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_w2v_trainable()*. Aquesta funció utilitza el model de Word2Vec per a convertir les frases a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train_trainable = map_w2v_trainable(wv_model, train, MAX_LEN, dictionary)\n",
    "mapped_test_trainable = map_w2v_trainable(wv_model, test, MAX_LEN, fixed_dictionary=dictionary)\n",
    "mapped_val_trainable = map_w2v_trainable(wv_model, val, MAX_LEN, fixed_dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partim en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trainable, y_train_trainable = pair_list_to_x_y(mapped_train_trainable)\n",
    "x_val_trainable, y_val_trainable = pair_list_to_x_y(mapped_val_trainable)\n",
    "x_test_trainable, y_test_trainable = pair_list_to_x_y(mapped_test_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformatejem les dades per a que siguin l'estructura correcte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_trainable: int = 64\n",
    "num_epochs_trainable: int = 128\n",
    "\n",
    "train_dataset_trainable = tf.data.Dataset.from_tensor_slices((x_train_trainable, y_train_trainable))\n",
    "train_dataset_trainable = train_dataset_trainable.shuffle(buffer_size=len(x_train_trainable)).batch(batch_size_trainable)\n",
    "\n",
    "val_dataset_trainable = tf.data.Dataset.from_tensor_slices((x_val_trainable, y_val_trainable))\n",
    "val_dataset_trainable = val_dataset_trainable.batch(batch_size_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el model amb embeddings aleatoris, sense cap tipus de preentrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 96)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 96)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, 96, 16)               16000     ['input_5[0][0]',             \n",
      "                                                                     'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " my_layer_mask (MyLayer_mas  (None, 96)                   0         ['input_5[0][0]']             \n",
      " k)                                                                                               \n",
      "                                                                                                  \n",
      " my_layer_mask_1 (MyLayer_m  (None, 96)                   0         ['input_6[0][0]']             \n",
      " ask)                                                                                             \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 96, 1)                289       ['embedding_2[0][0]',         \n",
      "                                                                     'embedding_2[1][0]']         \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4  (None, 96, 1)                0         ['my_layer_mask[0][0]']       \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5  (None, 96, 1)                0         ['my_layer_mask_1[0][0]']     \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " my_layer_exp (MyLayer_exp)  (None, 96, 1)                0         ['sequential_2[0][0]']        \n",
      "                                                                                                  \n",
      " my_layer_cast (MyLayer_cas  (None, 96, 1)                0         ['tf.__operators__.getitem_4[0\n",
      " t)                                                                 ][0]']                        \n",
      "                                                                                                  \n",
      " my_layer_exp_1 (MyLayer_ex  (None, 96, 1)                0         ['sequential_2[1][0]']        \n",
      " p)                                                                                               \n",
      "                                                                                                  \n",
      " my_layer_cast_1 (MyLayer_c  (None, 96, 1)                0         ['tf.__operators__.getitem_5[0\n",
      " ast)                                                               ][0]']                        \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLa  (None, 96, 1)                0         ['my_layer_exp[0][0]',        \n",
      " mbda)                                                               'my_layer_cast[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLa  (None, 96, 1)                0         ['my_layer_exp_1[0][0]',      \n",
      " mbda)                                                               'my_layer_cast_1[0][0]']     \n",
      "                                                                                                  \n",
      " my_layer_reduce_sum (MyLay  (None, 1, 1)                 0         ['tf.math.multiply_8[0][0]']  \n",
      " er_reduce_sum)                                                                                   \n",
      "                                                                                                  \n",
      " my_layer_reduce_sum_1 (MyL  (None, 1, 1)                 0         ['tf.math.multiply_9[0][0]']  \n",
      " ayer_reduce_sum)                                                                                 \n",
      "                                                                                                  \n",
      " tf.math.truediv_4 (TFOpLam  (None, 96, 1)                0         ['tf.math.multiply_8[0][0]',  \n",
      " bda)                                                                'my_layer_reduce_sum[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.truediv_5 (TFOpLam  (None, 96, 1)                0         ['tf.math.multiply_9[0][0]',  \n",
      " bda)                                                                'my_layer_reduce_sum_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpL  (None, 96, 16)               0         ['embedding_2[0][0]',         \n",
      " ambda)                                                              'tf.math.truediv_4[0][0]']   \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpL  (None, 96, 16)               0         ['embedding_2[1][0]',         \n",
      " ambda)                                                              'tf.math.truediv_5[0][0]']   \n",
      "                                                                                                  \n",
      " my_layer_reduce_sum_2 (MyL  (None, 1, 16)                0         ['tf.math.multiply_10[0][0]'] \n",
      " ayer_reduce_sum)                                                                                 \n",
      "                                                                                                  \n",
      " my_layer_reduce_sum_3 (MyL  (None, 1, 16)                0         ['tf.math.multiply_11[0][0]'] \n",
      " ayer_reduce_sum)                                                                                 \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)           (None, 1, 16)                0         ['my_layer_reduce_sum_2[0][0]'\n",
      "                                                                    , 'my_layer_reduce_sum_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 1, 16)                0         ['lambda_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1, 16)                272       ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 1, 16)                0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1, 1)                 17        ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)           (None, 1, 1)                 0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16578 (64.76 KB)\n",
      "Trainable params: 16578 (64.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_random = model_2(MAX_LEN,pretrained_weights=None, trainable=False, use_cosine=False)\n",
    "model_random.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node model_2/embedding_2/embedding_lookup defined at (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\ipykernel_6984\\1647691059.py\", line 1, in <module>\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1147, in train_step\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 588, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1047, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\__autograph_generated_filemh31qqft.py\", line 34, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 514, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 661, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 671, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1047, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\__autograph_generated_filemh31qqft.py\", line 34, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 225, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 263, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 273, in call\n\nindices[16,5] = 2327 is not in [0, 1000)\n\t [[{{node model_2/embedding_2/embedding_lookup}}]] [Op:__inference_train_function_4221]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_random\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset_trainable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model_2/embedding_2/embedding_lookup defined at (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 196, in _run_module_as_main\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 86, in _run_code\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\ipykernel_6984\\1647691059.py\", line 1, in <module>\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1147, in train_step\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 588, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1047, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\__autograph_generated_filemh31qqft.py\", line 34, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 514, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 661, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 663, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\functional.py\", line 671, in _run_internal_graph\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 553, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 558, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1047, in __call__\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\", line 1136, in __call__\n\n  File \"C:\\Users\\Pol\\AppData\\Local\\Temp\\__autograph_generated_filemh31qqft.py\", line 34, in error_handler\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 225, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 263, in call\n\n  File \"c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\layers\\core\\embedding.py\", line 273, in call\n\nindices[16,5] = 2327 is not in [0, 1000)\n\t [[{{node model_2/embedding_2/embedding_lookup}}]] [Op:__inference_train_function_4221]"
     ]
    }
   ],
   "source": [
    "model_random.fit(train_dataset_trainable, epochs=num_epochs_trainable, validation_data=val_dataset_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_random, x_train_trainable, y_train_trainable)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_random, x_val_trainable, y_val_trainable)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem amb la partició de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_random, x_test_trainable, y_test_trainable)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim els pesos preentrenats. Cal destacar que el model de Word2Vec *wv_model* ja ha estat importat en la secció [2. Word2Vec preentrenats](#2-word2vec-preentrenats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pretrained_weights = None\n",
    "if USE_PRETRAINED:\n",
    "    if REMAP_EMBEDDINGS:\n",
    "        _pretrained_weights = np.zeros(\n",
    "            (len(dictionary.token2id) + 1, wv_model.vector_size),  dtype=np.float32)\n",
    "        for token, _id in dictionary.token2id.items():\n",
    "            if token in wv_model:\n",
    "             _pretrained_weights[_id + 1] = wv_model[token]\n",
    "\n",
    "            else:\n",
    "                # In W2V, OOV will not have a representation. We will use 0.\n",
    "                pass\n",
    "    else:\n",
    "        # Not recommended (this will consume A LOT of RAM)\n",
    "        _pretrained_weights = np.zeros((wv_model.vectors.shape[0] + 1, wv_model.vector_size,),  dtype=np.float32)\n",
    "        _pretrained_weights[1:, :] = wv_model.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el model amb embeddings de Word2Vec preentrenats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model_2(MAX_LEN,pretrained_weights=_pretrained_weights, trainable=False, use_cosine=False)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(train_dataset_trainable, epochs=num_epochs_trainable, validation_data=val_dataset_trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(x_train_trainable, y_train_trainable, model2)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(x_val_trainable, y_val_trainable, model2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem amb la partició de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(x_test_trainable, y_test_trainable, model2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordEmbeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
