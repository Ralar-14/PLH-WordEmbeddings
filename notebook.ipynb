{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLH - Rubén Álvarez Aragonés i Pol Pérez Prades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.ca.examples import sentences \n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Requisites\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedder:\n",
    "    def __init__(self, corpus_path, corpus_size, load_model=False, model_path=None):\n",
    "        if not load_model:\n",
    "            self.corpus_path = corpus_path\n",
    "            self.corpus_size = int(corpus_size * 2**30) if corpus_size else None  # Convert GB to bytes\n",
    "            self.corpus = self.get_corpus(corpus_path)\n",
    "            self.fit()\n",
    "        else:\n",
    "            try:\n",
    "                self.load(model_path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"Model not found. Please check the path.\")\n",
    "                return\n",
    "\n",
    "    def get_corpus(self, corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            print(\"Reading corpus...\")\n",
    "            corpus = f.read(self.corpus_size) if self.corpus_size else f.read()\n",
    "            print(\"Preprocessing corpus...\")\n",
    "            corpus = self.preprocess(corpus)  # Preprocess the corpus and tokenize it\n",
    "        return corpus\n",
    "\n",
    "    def fit(self, window_size=15, vector_size=300, min_count=10, workers=8, epochs=10):\n",
    "        # Initialize the Word2Vec model with gensim\n",
    "        print(\"Initializing Word2Vec model...\")\n",
    "        self.model = word2vec.Word2Vec(sentences=[self.corpus], vector_size=vector_size, window=window_size, min_count=min_count, workers=workers, epochs=epochs)\n",
    "        print(\"Model training completed.\")\n",
    "\n",
    "    def save(self, model_path):\n",
    "        # Save the model\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        # Load the model\n",
    "        self.model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        # Lowercase the corpus\n",
    "        print(\"Lowercasing...\")\n",
    "        corpus = corpus.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        print(\"Removing special characters...\")\n",
    "        corpus = re.sub(r'[^a-záàéèíìóòúùñüç\\s]', ' ', corpus)\n",
    "        \n",
    "        # Tokenize the corpus\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus = nltk.word_tokenize(corpus)\n",
    "        \n",
    "        # Eliminate last token (probably incomplete word)\n",
    "        corpus = corpus[:-1]\n",
    "        \n",
    "        return corpus\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        # Get the embedding of a word\n",
    "        try:\n",
    "            return self.model.wv[word]\n",
    "        except KeyError:\n",
    "            print(f\"Word '{word}' not in vocabulary.\")\n",
    "            return None\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocabulary:\", list(self.model.wv.index_to_key))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model amb 100MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.save('models/word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.print_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.model.wv.most_similar(\"negre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model amb 500MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500.model.wv.most_similar(\"inshalla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model amb 1GB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_1024 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model amb totes les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_full_data = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importació_data import read_all_ts_data, reformat_data, create_corpus, preprocess, flattened_corpus_count, stopwords_cat\n",
    "from importació_data import pair_list_to_x_y\n",
    "from model_bàsic import build_and_compile_model_better\n",
    "import tensorflow as tf\n",
    "from model_bàsic import compute_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim les stopwords del català i la funció del preprocessament del text, que les tindrà en compte a la hora de tokenitzar i natejar el text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpw_cat = stopwords_cat()\n",
    "prepro = lambda x: preprocess(x, stpw_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per altra banda, llegim les dades i definim les variables més importants per a la creació dels models de similitud de text semàntic.\n",
    "- Llegim totes les dades de text similarity dividint-les en train, test i val. \n",
    "- Reformatejem les dades per a que siguin l'estructura List[Tuple[str, str, float]]. \n",
    "- Definim el corpus i el diccionari amb totes les paraules.\n",
    "- Creem un diccionari de python amb tots els indexs com a claus i amb la repetició de les paraules com a valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = read_all_ts_data()\n",
    "train, test, val = reformat_data(train, test, val)\n",
    "corpus, dictionary = create_corpus(train, test, val, preprocess=prepro)\n",
    "flat_corpus = flattened_corpus_count(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compartació amb diferents models de Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onehot import map_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un embedding OneHot té tamany igual a la llargada del diccionari. En la importació de dades ja hem eliminat les stopwords per reduïr la dimensió, però ara també eliminarem del embedding aquelles paraules que es repeteixen masses poques vegades o massa sovint. \n",
    "\n",
    "Per aconseguir això creem una llista que conté els indexs de les paraules que sí que utilitzarem i la passem com a argument a la funció *map_one_hot()*, per reduïr la dimensió del embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate from the dictionary the words that are repeated very few times or too many times\n",
    "keys_preprocess = [index  for index in dictionary if flat_corpus[index] > 10 and flat_corpus[index] < 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim les paraules a vectors OneHot amb la funció *map_one_hot()*. Aquesta funció crea un vector de zeros de la mida del diccionari i posa un 1 a la posició de la paraula en el diccionari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_one_hot_train = map_one_hot(train, dictionary, keys_preprocess)\n",
    "mapped_one_hot_test = map_one_hot(test, dictionary, keys_preprocess)\n",
    "mapped_one_hot_val = map_one_hot(val, dictionary, keys_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem el X i Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh, y_train_oh = pair_list_to_x_y(mapped_one_hot_train)\n",
    "x_val_oh, y_val_oh = pair_list_to_x_y(mapped_one_hot_val)\n",
    "x_test_oh, y_test_oh = pair_list_to_x_y(mapped_one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_oh: int = 64\n",
    "num_epochs_oh: int = 64\n",
    "\n",
    "train_dataset_oh = tf.data.Dataset.from_tensor_slices((x_train_oh, y_train_oh))\n",
    "train_dataset_oh = train_dataset_oh.shuffle(buffer_size=len(x_train_oh)).batch(batch_size_oh)\n",
    "\n",
    "val_dataset_oh = tf.data.Dataset.from_tensor_slices((x_val_oh, y_val_oh))\n",
    "val_dataset_oh = val_dataset_oh.batch(batch_size_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_18      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_19      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">642,402</span> │ input_layer_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,832</span> │ dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_18      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_19      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │    \u001b[38;5;34m642,402\u001b[0m │ input_layer_18[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_19[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_18 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_27[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lambda_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m12,832\u001b[0m │ dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_19 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">655,251</span> (2.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m655,251\u001b[0m (2.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">655,251</span> (2.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m655,251\u001b[0m (2.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size_oh = len(keys_preprocess)\n",
    "model_oh = build_and_compile_model_better(embedding_size = embedding_size_oh)\n",
    "tf.keras.utils.plot_model(model_oh, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_oh.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.7308 - val_loss: 0.7461\n",
      "Epoch 2/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7177 - val_loss: 0.7421\n",
      "Epoch 3/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7093 - val_loss: 0.7395\n",
      "Epoch 4/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6940 - val_loss: 0.7364\n",
      "Epoch 5/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.6724 - val_loss: 0.7320\n",
      "Epoch 6/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6235 - val_loss: 0.7303\n",
      "Epoch 7/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5342 - val_loss: 0.7446\n",
      "Epoch 8/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4239 - val_loss: 0.7709\n",
      "Epoch 9/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3430 - val_loss: 0.7955\n",
      "Epoch 10/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2663 - val_loss: 0.8151\n",
      "Epoch 11/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2242 - val_loss: 0.8289\n",
      "Epoch 12/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1998 - val_loss: 0.8385\n",
      "Epoch 13/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1833 - val_loss: 0.8442\n",
      "Epoch 14/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1627 - val_loss: 0.8502\n",
      "Epoch 15/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1613 - val_loss: 0.8503\n",
      "Epoch 16/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1465 - val_loss: 0.8682\n",
      "Epoch 17/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1368 - val_loss: 0.8654\n",
      "Epoch 18/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1297 - val_loss: 0.8660\n",
      "Epoch 19/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1292 - val_loss: 0.8595\n",
      "Epoch 20/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1221 - val_loss: 0.8657\n",
      "Epoch 21/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1224 - val_loss: 0.8636\n",
      "Epoch 22/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1155 - val_loss: 0.8651\n",
      "Epoch 23/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1080 - val_loss: 0.8660\n",
      "Epoch 24/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1100 - val_loss: 0.8689\n",
      "Epoch 25/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1051 - val_loss: 0.8722\n",
      "Epoch 26/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1024 - val_loss: 0.8723\n",
      "Epoch 27/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0981 - val_loss: 0.8613\n",
      "Epoch 28/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1000 - val_loss: 0.8611\n",
      "Epoch 29/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0958 - val_loss: 0.8666\n",
      "Epoch 30/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0993 - val_loss: 0.8685\n",
      "Epoch 31/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0941 - val_loss: 0.8691\n",
      "Epoch 32/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0907 - val_loss: 0.8701\n",
      "Epoch 33/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0875 - val_loss: 0.8667\n",
      "Epoch 34/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0823 - val_loss: 0.8711\n",
      "Epoch 35/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0836 - val_loss: 0.8625\n",
      "Epoch 36/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0799 - val_loss: 0.8701\n",
      "Epoch 37/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0797 - val_loss: 0.8670\n",
      "Epoch 38/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0809 - val_loss: 0.8659\n",
      "Epoch 39/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0811 - val_loss: 0.8589\n",
      "Epoch 40/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0812 - val_loss: 0.8574\n",
      "Epoch 41/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0775 - val_loss: 0.8616\n",
      "Epoch 42/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0825 - val_loss: 0.8594\n",
      "Epoch 43/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0792 - val_loss: 0.8688\n",
      "Epoch 44/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0781 - val_loss: 0.8643\n",
      "Epoch 45/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0790 - val_loss: 0.8638\n",
      "Epoch 46/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0766 - val_loss: 0.8627\n",
      "Epoch 47/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0734 - val_loss: 0.8676\n",
      "Epoch 48/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0743 - val_loss: 0.8612\n",
      "Epoch 49/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0758 - val_loss: 0.8600\n",
      "Epoch 50/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0740 - val_loss: 0.8657\n",
      "Epoch 51/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0781 - val_loss: 0.8658\n",
      "Epoch 52/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0743 - val_loss: 0.8626\n",
      "Epoch 53/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0770 - val_loss: 0.8598\n",
      "Epoch 54/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0764 - val_loss: 0.8642\n",
      "Epoch 55/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0717 - val_loss: 0.8652\n",
      "Epoch 56/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0711 - val_loss: 0.8629\n",
      "Epoch 57/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0749 - val_loss: 0.8626\n",
      "Epoch 58/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0718 - val_loss: 0.8614\n",
      "Epoch 59/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0731 - val_loss: 0.8651\n",
      "Epoch 60/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0753 - val_loss: 0.8695\n",
      "Epoch 61/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0739 - val_loss: 0.8679\n",
      "Epoch 62/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0730 - val_loss: 0.8675\n",
      "Epoch 63/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0742 - val_loss: 0.8683\n",
      "Epoch 64/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0739 - val_loss: 0.8691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b1fdb22410>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_oh.fit(train_dataset_oh, epochs=num_epochs_oh, validation_data=val_dataset_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Correlación de Pearson (train): 0.9564202112077503\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Correlación de Pearson (validation): 0.16723982892041112\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_oh, x_train_oh, y_train_oh)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_oh, x_val_oh, y_val_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Correlación de Pearson (test): 0.213597538697998\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_oh, x_test_oh, y_test_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de One-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com podem observar a la hora de fer l'entrenament, el model fa molt *overfit*, així que no és capaç de generalitzar bé. Això és degut a que el model OneHot no té en compte la semàntica de les paraules, ja que cada paraula és tractada com a un element únic i no es té en compte el context en el que apareixen les paraules. Una altra raó per la que el model OneHot no és bó és perquè la mida dels vectors és molt gran, ja que la mida dels vectors és igual a la mida del diccionari, i això fa que el model sigui molt ineficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec preentrenats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_tf_idf import map_pairs_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegim el model de Word2Vec preentrenat en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_FILE = \"C:/Users/Pol/Downloads/cc.ca.300.bin.gz\"\n",
    "\n",
    "\n",
    "USE_MMAP = False\n",
    "if USE_MMAP:\n",
    "    from gensim.models.fasttext import FastTextKeyedVectors\n",
    "    MMAP_PATH = 'cc.ca.300.bin'\n",
    "    # wv_model.save(MMAP_PATH)\n",
    "    wv_model = FastTextKeyedVectors.load(MMAP_PATH, mmap='r')\n",
    "else:\n",
    "    from gensim.models import fasttext\n",
    "    wv_model = fasttext.load_facebook_vectors(WORD_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Mitjana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim a vectors les frases amb la funció *map_w2v()*. Aquesta funció obté l'embedding de Word2Vec de cada paraula de la frase i en fa la mitjana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_mean_train = map_pairs_w2v(train, wv_model, dictionary=dictionary, preprocess=prepro)\n",
    "mapped_w2v_mean_test = map_pairs_w2v(test, wv_model, dictionary=dictionary, preprocess=prepro)\n",
    "mapped_w2v_mean_val = map_pairs_w2v(val, wv_model, dictionary=dictionary, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem les dades en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_mean, y_train_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_train)\n",
    "x_val_w2v_mean, y_val_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_val)\n",
    "x_test_w2v_mean, y_test_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_mean: int = 64\n",
    "num_epochs_w2v_mean: int = 64\n",
    "\n",
    "train_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_train_w2v_mean, y_train_w2v_mean))\n",
    "train_dataset_w2v_mean = train_dataset_w2v_mean.shuffle(buffer_size=len(x_train_w2v_mean)).batch(batch_size_w2v_mean)\n",
    "\n",
    "val_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_val_w2v_mean, y_val_w2v_mean))\n",
    "val_dataset_w2v_mean = val_dataset_w2v_mean.batch(batch_size_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_21      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │ input_layer_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,816</span> │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_21      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │     \u001b[38;5;34m90,300\u001b[0m │ input_layer_20[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_21[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_20 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_30[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lambda_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,816\u001b[0m │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_21 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size_w2v_mean = 300\n",
    "model_w2v_mean = build_and_compile_model_better(embedding_size = embedding_size_w2v_mean)\n",
    "tf.keras.utils.plot_model(model_w2v_mean, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_mean.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 72ms/step - loss: 0.7246 - val_loss: 0.7352\n",
      "Epoch 2/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.7140 - val_loss: 0.7181\n",
      "Epoch 3/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.6893 - val_loss: 0.6964\n",
      "Epoch 4/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.6537 - val_loss: 0.6788\n",
      "Epoch 5/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.6067 - val_loss: 0.6633\n",
      "Epoch 6/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.5570 - val_loss: 0.6565\n",
      "Epoch 7/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.4884 - val_loss: 0.6457\n",
      "Epoch 8/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.4191 - val_loss: 0.6338\n",
      "Epoch 9/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.3542 - val_loss: 0.6304\n",
      "Epoch 10/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.2989 - val_loss: 0.6156\n",
      "Epoch 11/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.2593 - val_loss: 0.6262\n",
      "Epoch 12/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2138 - val_loss: 0.6234\n",
      "Epoch 13/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1917 - val_loss: 0.6425\n",
      "Epoch 14/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1736 - val_loss: 0.6450\n",
      "Epoch 15/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1563 - val_loss: 0.6611\n",
      "Epoch 16/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1471 - val_loss: 0.6882\n",
      "Epoch 17/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1512 - val_loss: 0.6650\n",
      "Epoch 18/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1342 - val_loss: 0.6954\n",
      "Epoch 19/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1379 - val_loss: 0.6961\n",
      "Epoch 20/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1392 - val_loss: 0.6681\n",
      "Epoch 21/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1268 - val_loss: 0.7195\n",
      "Epoch 22/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1357 - val_loss: 0.6889\n",
      "Epoch 23/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1251 - val_loss: 0.6864\n",
      "Epoch 24/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1064 - val_loss: 0.6651\n",
      "Epoch 25/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0946 - val_loss: 0.6767\n",
      "Epoch 26/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.0929 - val_loss: 0.6786\n",
      "Epoch 27/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0855 - val_loss: 0.6492\n",
      "Epoch 28/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0848 - val_loss: 0.6593\n",
      "Epoch 29/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0885 - val_loss: 0.6567\n",
      "Epoch 30/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0920 - val_loss: 0.6710\n",
      "Epoch 31/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0844 - val_loss: 0.6674\n",
      "Epoch 32/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0885 - val_loss: 0.6892\n",
      "Epoch 33/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0794 - val_loss: 0.6834\n",
      "Epoch 34/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0771 - val_loss: 0.6776\n",
      "Epoch 35/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0781 - val_loss: 0.6944\n",
      "Epoch 36/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0746 - val_loss: 0.7014\n",
      "Epoch 37/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0727 - val_loss: 0.6919\n",
      "Epoch 38/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - loss: 0.0718 - val_loss: 0.6782\n",
      "Epoch 39/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0710 - val_loss: 0.6798\n",
      "Epoch 40/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0705 - val_loss: 0.6927\n",
      "Epoch 41/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0632 - val_loss: 0.6982\n",
      "Epoch 42/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0669 - val_loss: 0.7035\n",
      "Epoch 43/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0663 - val_loss: 0.6944\n",
      "Epoch 44/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0696 - val_loss: 0.6710\n",
      "Epoch 45/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.0627 - val_loss: 0.6667\n",
      "Epoch 46/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0648 - val_loss: 0.6815\n",
      "Epoch 47/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0616 - val_loss: 0.6675\n",
      "Epoch 48/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0624 - val_loss: 0.6799\n",
      "Epoch 49/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0644 - val_loss: 0.6774\n",
      "Epoch 50/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0519 - val_loss: 0.6893\n",
      "Epoch 51/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0587 - val_loss: 0.6542\n",
      "Epoch 52/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0575 - val_loss: 0.6637\n",
      "Epoch 53/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0567 - val_loss: 0.6684\n",
      "Epoch 54/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0588 - val_loss: 0.6725\n",
      "Epoch 55/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0503 - val_loss: 0.6721\n",
      "Epoch 56/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0492 - val_loss: 0.6880\n",
      "Epoch 57/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0522 - val_loss: 0.6938\n",
      "Epoch 58/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0462 - val_loss: 0.6893\n",
      "Epoch 59/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0483 - val_loss: 0.6724\n",
      "Epoch 60/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0451 - val_loss: 0.6727\n",
      "Epoch 61/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0434 - val_loss: 0.6690\n",
      "Epoch 62/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0450 - val_loss: 0.6581\n",
      "Epoch 63/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 0.0414 - val_loss: 0.6592\n",
      "Epoch 64/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0382 - val_loss: 0.6591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b40bf75090>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v_mean.fit(train_dataset_w2v_mean, epochs=num_epochs_w2v_mean, validation_data=val_dataset_w2v_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n",
      "Correlación de Pearson (train): 0.9872606130377132\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Correlación de Pearson (validation): 0.3928132701809035\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_mean, x_train_w2v_mean, y_train_w2v_mean)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_mean, x_val_w2v_mean, y_val_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Correlación de Pearson (test): 0.33375648147162784\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_mean, x_test_w2v_mean, y_test_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de Word2Vec preentrenats amb mitjana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem observar com amb el model Word2vec i la mitjana el model segueix fent sobreajust però funciona molt millor que el OneHot. Això és degut a que el model Word2Vec té en compte la semàntica de les paraules, ja que les paraules que tenen un significat similar tenen un embedding similar. Això fa que el model sigui capaç de generalitzar millor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim el model de Tf-idf amb el corpus definit al inici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "modelo_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fent servir la funció *map_w2v_tfidf()* s'obté l'embedding de Word2Vec de cada paraula de la frase i s'aplica Tf-idf per obtenir un vector de la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_tfidf_train = map_pairs_w2v(train, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_test = map_pairs_w2v(test, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_val = map_pairs_w2v(val, wv_model, dictionary=dictionary,tf_idf_model=modelo_tfidf, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem les dades en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_tfidf, y_train_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_train)\n",
    "x_val_w2v_tfidf, y_val_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_val)\n",
    "x_test_w2v_tfidf, y_test_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_tfidf: int = 64\n",
    "num_epochs_w2v_tfidf: int = 64\n",
    "\n",
    "train_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_train_w2v_tfidf, y_train_w2v_tfidf))\n",
    "train_dataset_w2v_tfidf = train_dataset_w2v_tfidf.shuffle(buffer_size=len(x_train_w2v_tfidf)).batch(batch_size_w2v_tfidf)\n",
    "\n",
    "val_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_val_w2v_tfidf, y_val_w2v_tfidf))\n",
    "val_dataset_w2v_tfidf = val_dataset_w2v_tfidf.batch(batch_size_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_23\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_23\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,816</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_23      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │     \u001b[38;5;34m90,300\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_23[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_22 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_33[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lambda_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,816\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_23 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size_w2v_tfidf = 300\n",
    "model_w2v_tfidf = build_and_compile_model_better(embedding_size = embedding_size_w2v_tfidf)\n",
    "tf.keras.utils.plot_model(model_w2v_tfidf, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_tfidf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 0.7239 - val_loss: 0.7336\n",
      "Epoch 2/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.7077 - val_loss: 0.7137\n",
      "Epoch 3/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.6876 - val_loss: 0.6926\n",
      "Epoch 4/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.6460 - val_loss: 0.6725\n",
      "Epoch 5/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.5930 - val_loss: 0.6535\n",
      "Epoch 6/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.5131 - val_loss: 0.6400\n",
      "Epoch 7/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4231 - val_loss: 0.6248\n",
      "Epoch 8/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3334 - val_loss: 0.6086\n",
      "Epoch 9/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2712 - val_loss: 0.6021\n",
      "Epoch 10/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2226 - val_loss: 0.5961\n",
      "Epoch 11/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.1871 - val_loss: 0.6069\n",
      "Epoch 12/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.1499 - val_loss: 0.6014\n",
      "Epoch 13/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1340 - val_loss: 0.6192\n",
      "Epoch 14/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1194 - val_loss: 0.6632\n",
      "Epoch 15/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.1290 - val_loss: 0.6883\n",
      "Epoch 16/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.1372 - val_loss: 0.7243\n",
      "Epoch 17/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1278 - val_loss: 0.6331\n",
      "Epoch 18/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.1427 - val_loss: 0.6665\n",
      "Epoch 19/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1133 - val_loss: 0.6053\n",
      "Epoch 20/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0997 - val_loss: 0.6247\n",
      "Epoch 21/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0890 - val_loss: 0.6163\n",
      "Epoch 22/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0832 - val_loss: 0.6176\n",
      "Epoch 23/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0877 - val_loss: 0.6177\n",
      "Epoch 24/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0769 - val_loss: 0.6152\n",
      "Epoch 25/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0719 - val_loss: 0.6182\n",
      "Epoch 26/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0750 - val_loss: 0.6149\n",
      "Epoch 27/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - loss: 0.0763 - val_loss: 0.6079\n",
      "Epoch 28/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0729 - val_loss: 0.6132\n",
      "Epoch 29/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - loss: 0.0674 - val_loss: 0.6131\n",
      "Epoch 30/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0668 - val_loss: 0.6094\n",
      "Epoch 31/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0705 - val_loss: 0.6100\n",
      "Epoch 32/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0670 - val_loss: 0.6128\n",
      "Epoch 33/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.0703 - val_loss: 0.6292\n",
      "Epoch 34/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0644 - val_loss: 0.6080\n",
      "Epoch 35/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0637 - val_loss: 0.6298\n",
      "Epoch 36/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0631 - val_loss: 0.6140\n",
      "Epoch 37/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0611 - val_loss: 0.6090\n",
      "Epoch 38/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0629 - val_loss: 0.6101\n",
      "Epoch 39/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0613 - val_loss: 0.6117\n",
      "Epoch 40/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0596 - val_loss: 0.6296\n",
      "Epoch 41/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.0584 - val_loss: 0.6706\n",
      "Epoch 42/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0647 - val_loss: 0.6626\n",
      "Epoch 43/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.0645 - val_loss: 0.6802\n",
      "Epoch 44/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0695 - val_loss: 0.6686\n",
      "Epoch 45/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0595 - val_loss: 0.6914\n",
      "Epoch 46/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0643 - val_loss: 0.6746\n",
      "Epoch 47/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0711 - val_loss: 0.6548\n",
      "Epoch 48/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0734 - val_loss: 0.6430\n",
      "Epoch 49/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0634 - val_loss: 0.6076\n",
      "Epoch 50/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0733 - val_loss: 0.6122\n",
      "Epoch 51/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0691 - val_loss: 0.6166\n",
      "Epoch 52/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0630 - val_loss: 0.6228\n",
      "Epoch 53/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0672 - val_loss: 0.6310\n",
      "Epoch 54/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0597 - val_loss: 0.6172\n",
      "Epoch 55/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0539 - val_loss: 0.6200\n",
      "Epoch 56/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0461 - val_loss: 0.6130\n",
      "Epoch 57/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0397 - val_loss: 0.5920\n",
      "Epoch 58/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0396 - val_loss: 0.6021\n",
      "Epoch 59/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0333 - val_loss: 0.5989\n",
      "Epoch 60/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.0348 - val_loss: 0.5934\n",
      "Epoch 61/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0328 - val_loss: 0.6079\n",
      "Epoch 62/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0325 - val_loss: 0.6063\n",
      "Epoch 63/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0320 - val_loss: 0.6099\n",
      "Epoch 64/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0282 - val_loss: 0.6078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b40acccfa0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v_tfidf.fit(train_dataset_w2v_tfidf, epochs=num_epochs_w2v_tfidf, validation_data=val_dataset_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
      "Correlación de Pearson (train): 0.9932394105611743\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Correlación de Pearson (validation): 0.45649112450570667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_tfidf, x_train_w2v_tfidf, y_train_w2v_tfidf)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_tfidf, x_val_w2v_tfidf, y_val_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Correlación de Pearson (test): 0.3930336805674309\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_tfidf, x_test_w2v_tfidf, y_test_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de Word2Vec preentrenats amb Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem observar com, fent us de Tf-idf, el model millora una mica més. Això és degut a que Tf-idf té en compte la freqüència de les paraules en el corpus, i això fa que les paraules que apareixen molt sovint tinguin un pes més baix. Això fa que el model sigui capaç de generalitzar millor. Tot i així el model segueix sobreajustant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importem tot allò necessari per a fer servir el model de SpaCy en català, inclòs el model preentrenat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_embed import map_spacy_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-md==3.7.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_md-3.7.0/ca_core_news_md-3.7.0-py3-none-any.whl (49.2 MB)\n",
      "     ---------------------------------------- 0.0/49.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/49.2 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------------------- 0.4/49.2 MB 3.3 MB/s eta 0:00:15\n",
      "      --------------------------------------- 0.9/49.2 MB 5.7 MB/s eta 0:00:09\n",
      "      --------------------------------------- 1.2/49.2 MB 6.0 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 1.4/49.2 MB 5.7 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.8/49.2 MB 5.4 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 2.5/49.2 MB 6.5 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 2.8/49.2 MB 6.7 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 3.4/49.2 MB 7.1 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.0/49.2 MB 7.3 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.8/49.2 MB 7.7 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 5.3/49.2 MB 7.7 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 5.9/49.2 MB 7.8 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.4/49.2 MB 7.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.8/49.2 MB 7.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 7.3/49.2 MB 7.9 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.7/49.2 MB 7.9 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.9/49.2 MB 7.8 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 8.3/49.2 MB 7.4 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.7/49.2 MB 7.4 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 9.3/49.2 MB 7.5 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 10.0/49.2 MB 7.7 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 10.5/49.2 MB 8.1 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 10.9/49.2 MB 8.1 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 11.4/49.2 MB 7.8 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 11.9/49.2 MB 7.8 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 12.4/49.2 MB 7.9 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 12.7/49.2 MB 7.6 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 13.0/49.2 MB 7.4 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 13.4/49.2 MB 7.1 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 13.8/49.2 MB 7.0 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 14.3/49.2 MB 7.0 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 14.6/49.2 MB 6.7 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 15.2/49.2 MB 6.6 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 15.7/49.2 MB 6.6 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 16.2/49.2 MB 6.5 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 16.6/49.2 MB 6.3 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 17.1/49.2 MB 6.2 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 17.7/49.2 MB 6.3 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 18.4/49.2 MB 6.5 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 19.1/49.2 MB 6.6 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 19.8/49.2 MB 6.7 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 20.3/49.2 MB 6.6 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 20.8/49.2 MB 6.7 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 21.4/49.2 MB 6.7 MB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 21.7/49.2 MB 6.7 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 22.2/49.2 MB 6.7 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 22.6/49.2 MB 6.7 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 23.1/49.2 MB 6.7 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 23.9/49.2 MB 7.2 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 24.7/49.2 MB 7.7 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 25.3/49.2 MB 7.7 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 26.2/49.2 MB 8.1 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 26.8/49.2 MB 8.5 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 27.5/49.2 MB 8.8 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 28.1/49.2 MB 8.8 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 29.1/49.2 MB 9.2 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 30.1/49.2 MB 9.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 30.7/49.2 MB 9.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 31.3/49.2 MB 9.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 31.7/49.2 MB 9.9 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 32.4/49.2 MB 10.4 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 33.0/49.2 MB 10.9 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 33.7/49.2 MB 10.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 34.1/49.2 MB 10.2 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 35.0/49.2 MB 10.6 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 35.3/49.2 MB 9.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 35.9/49.2 MB 9.8 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 36.5/49.2 MB 9.5 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 37.1/49.2 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 37.4/49.2 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 38.0/49.2 MB 8.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 38.3/49.2 MB 7.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 38.8/49.2 MB 7.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 39.5/49.2 MB 6.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 39.9/49.2 MB 6.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 40.2/49.2 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 40.5/49.2 MB 5.5 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 40.8/49.2 MB 5.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 41.4/49.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 41.9/49.2 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 42.4/49.2 MB 5.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 42.8/49.2 MB 5.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 43.3/49.2 MB 5.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 43.9/49.2 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 44.3/49.2 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 44.8/49.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 45.1/49.2 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 45.9/49.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 46.2/49.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 46.5/49.2 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 46.9/49.2 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 47.3/49.2 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 47.9/49.2 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  48.5/49.2 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  48.9/49.2 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  49.2/49.2 MB 5.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 49.2/49.2 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from ca-core-news-md==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.7.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (4.12.1)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.18.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-md==3.7.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model de SpaCy en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_spacy()*. Aquesta funció utilitza el model de SpaCy per a convertir les frases a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_spacy_train = map_spacy_embed(train, spacy_model)\n",
    "mapped_spacy_test = map_spacy_embed(test, spacy_model)\n",
    "mapped_spacy_val = map_spacy_embed(val, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividim les particions en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sp, y_train_sp = pair_list_to_x_y(mapped_spacy_train)\n",
    "x_val_sp, y_val_sp = pair_list_to_x_y(mapped_spacy_val)\n",
    "x_test_sp, y_test_sp = pair_list_to_x_y(mapped_spacy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_sp: int = 64\n",
    "num_epochs_sp: int = 64\n",
    "\n",
    "train_dataset_sp = tf.data.Dataset.from_tensor_slices((x_train_sp, y_train_sp))\n",
    "train_dataset_sp = train_dataset_sp.shuffle(buffer_size=len(x_train_sp)).batch(batch_size_sp)\n",
    "\n",
    "val_dataset_sp = tf.data.Dataset.from_tensor_slices((x_val_sp, y_val_sp))\n",
    "val_dataset_sp = val_dataset_sp.batch(batch_size_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_25      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │ input_layer_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,816</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_24      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_25      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │     \u001b[38;5;34m90,300\u001b[0m │ input_layer_24[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_25[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_24 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_36[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lambda_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │      \u001b[38;5;34m4,816\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_25 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">95,133</span> (371.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m95,133\u001b[0m (371.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size_sp = 300\n",
    "model_sp = build_and_compile_model_better(embedding_size = embedding_size_sp)\n",
    "tf.keras.utils.plot_model(model_sp, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_sp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.7256 - val_loss: 0.7358\n",
      "Epoch 2/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7120 - val_loss: 0.7282\n",
      "Epoch 3/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6974 - val_loss: 0.7170\n",
      "Epoch 4/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6781 - val_loss: 0.7059\n",
      "Epoch 5/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.6569 - val_loss: 0.6943\n",
      "Epoch 6/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.6208 - val_loss: 0.6850\n",
      "Epoch 7/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6022 - val_loss: 0.6777\n",
      "Epoch 8/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5681 - val_loss: 0.6768\n",
      "Epoch 9/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5407 - val_loss: 0.6573\n",
      "Epoch 10/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.5137 - val_loss: 0.6573\n",
      "Epoch 11/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.4758 - val_loss: 0.6420\n",
      "Epoch 12/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4558 - val_loss: 0.6342\n",
      "Epoch 13/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.4213 - val_loss: 0.6371\n",
      "Epoch 14/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3911 - val_loss: 0.6316\n",
      "Epoch 15/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.3725 - val_loss: 0.6383\n",
      "Epoch 16/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.3462 - val_loss: 0.6533\n",
      "Epoch 17/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.3272 - val_loss: 0.6540\n",
      "Epoch 18/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3094 - val_loss: 0.6548\n",
      "Epoch 19/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3007 - val_loss: 0.6512\n",
      "Epoch 20/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2950 - val_loss: 0.6677\n",
      "Epoch 21/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2718 - val_loss: 0.6688\n",
      "Epoch 22/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2456 - val_loss: 0.6732\n",
      "Epoch 23/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2449 - val_loss: 0.6697\n",
      "Epoch 24/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2319 - val_loss: 0.6617\n",
      "Epoch 25/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2260 - val_loss: 0.6943\n",
      "Epoch 26/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2175 - val_loss: 0.7025\n",
      "Epoch 27/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2137 - val_loss: 0.6842\n",
      "Epoch 28/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2075 - val_loss: 0.6982\n",
      "Epoch 29/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2023 - val_loss: 0.7116\n",
      "Epoch 30/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1880 - val_loss: 0.6970\n",
      "Epoch 31/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1868 - val_loss: 0.6967\n",
      "Epoch 32/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1788 - val_loss: 0.7106\n",
      "Epoch 33/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1738 - val_loss: 0.7232\n",
      "Epoch 34/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1955 - val_loss: 0.7446\n",
      "Epoch 35/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1778 - val_loss: 0.7433\n",
      "Epoch 36/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1629 - val_loss: 0.8047\n",
      "Epoch 37/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1822 - val_loss: 0.8250\n",
      "Epoch 38/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1729 - val_loss: 0.7482\n",
      "Epoch 39/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1621 - val_loss: 0.6995\n",
      "Epoch 40/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1621 - val_loss: 0.7825\n",
      "Epoch 41/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1654 - val_loss: 0.7803\n",
      "Epoch 42/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1646 - val_loss: 0.7810\n",
      "Epoch 43/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1904 - val_loss: 0.7535\n",
      "Epoch 44/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1939 - val_loss: 0.6709\n",
      "Epoch 45/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1730 - val_loss: 0.6966\n",
      "Epoch 46/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1619 - val_loss: 0.7182\n",
      "Epoch 47/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1430 - val_loss: 0.7204\n",
      "Epoch 48/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1288 - val_loss: 0.7438\n",
      "Epoch 49/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1265 - val_loss: 0.7677\n",
      "Epoch 50/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1262 - val_loss: 0.7744\n",
      "Epoch 51/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1255 - val_loss: 0.7800\n",
      "Epoch 52/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1275 - val_loss: 0.7703\n",
      "Epoch 53/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1335 - val_loss: 0.7701\n",
      "Epoch 54/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1251 - val_loss: 0.7318\n",
      "Epoch 55/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1163 - val_loss: 0.7608\n",
      "Epoch 56/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1156 - val_loss: 0.7563\n",
      "Epoch 57/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1071 - val_loss: 0.7654\n",
      "Epoch 58/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1155 - val_loss: 0.7597\n",
      "Epoch 59/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1066 - val_loss: 0.7573\n",
      "Epoch 60/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1058 - val_loss: 0.7379\n",
      "Epoch 61/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1061 - val_loss: 0.7495\n",
      "Epoch 62/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0994 - val_loss: 0.7620\n",
      "Epoch 63/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1019 - val_loss: 0.7722\n",
      "Epoch 64/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1023 - val_loss: 0.7762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b40a88f340>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sp.fit(train_dataset_sp, epochs=num_epochs_sp, validation_data=val_dataset_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb al partició de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Correlación de Pearson (train): 0.9482275546865951\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Correlación de Pearson (validation): 0.324149932695841\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_train_sp, y_train_sp)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_sp, x_val_sp, y_val_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Correlación de Pearson (train): 0.2461349343045748\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_test_sp, y_test_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions de SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilitzant el model de SpaCy, el model segueix fent sobreajust, i funciona de manera molt similar al Word2Vec, però una mica pitjor. Això és degut a que el model de SpaCy també té en compte la semàntica de les paraules, ja que les paraules que tenen un significat similar tenen un embedding similar. Això fa que el model sigui capaç de generalitzar millor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roberta_base import map_roberta_embed\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ca-core-news-trf==3.7.2\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_trf-3.7.2/ca_core_news_trf-3.7.2-py3-none-any.whl (457.1 MB)\n",
      "     ---------------------------------------- 0.0/457.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/457.1 MB 3.9 MB/s eta 0:01:57\n",
      "     ---------------------------------------- 0.5/457.1 MB 5.2 MB/s eta 0:01:28\n",
      "     ---------------------------------------- 0.9/457.1 MB 6.5 MB/s eta 0:01:10\n",
      "     ---------------------------------------- 1.4/457.1 MB 7.2 MB/s eta 0:01:03\n",
      "     ---------------------------------------- 1.8/457.1 MB 7.6 MB/s eta 0:01:01\n",
      "     ---------------------------------------- 2.4/457.1 MB 8.3 MB/s eta 0:00:55\n",
      "     ---------------------------------------- 2.7/457.1 MB 8.2 MB/s eta 0:00:56\n",
      "     ---------------------------------------- 3.2/457.1 MB 8.5 MB/s eta 0:00:54\n",
      "     ---------------------------------------- 3.7/457.1 MB 8.9 MB/s eta 0:00:52\n",
      "     ---------------------------------------- 4.1/457.1 MB 9.1 MB/s eta 0:00:50\n",
      "     ---------------------------------------- 4.7/457.1 MB 9.1 MB/s eta 0:00:50\n",
      "     ---------------------------------------- 5.2/457.1 MB 9.4 MB/s eta 0:00:48\n",
      "     ---------------------------------------- 5.7/457.1 MB 9.3 MB/s eta 0:00:49\n",
      "      --------------------------------------- 6.3/457.1 MB 9.7 MB/s eta 0:00:47\n",
      "      --------------------------------------- 6.7/457.1 MB 9.7 MB/s eta 0:00:47\n",
      "      --------------------------------------- 7.2/457.1 MB 9.6 MB/s eta 0:00:48\n",
      "      --------------------------------------- 7.7/457.1 MB 9.9 MB/s eta 0:00:46\n",
      "      -------------------------------------- 8.4/457.1 MB 10.0 MB/s eta 0:00:45\n",
      "      -------------------------------------- 9.0/457.1 MB 10.3 MB/s eta 0:00:44\n",
      "      -------------------------------------- 9.6/457.1 MB 10.2 MB/s eta 0:00:44\n",
      "      ------------------------------------- 10.3/457.1 MB 10.7 MB/s eta 0:00:42\n",
      "      ------------------------------------- 11.0/457.1 MB 11.3 MB/s eta 0:00:40\n",
      "      ------------------------------------- 11.6/457.1 MB 11.5 MB/s eta 0:00:39\n",
      "     - ------------------------------------ 12.2/457.1 MB 12.1 MB/s eta 0:00:37\n",
      "     - ------------------------------------ 13.0/457.1 MB 12.4 MB/s eta 0:00:36\n",
      "     - ------------------------------------ 13.5/457.1 MB 12.1 MB/s eta 0:00:37\n",
      "     - ------------------------------------ 14.3/457.1 MB 12.8 MB/s eta 0:00:35\n",
      "     - ------------------------------------ 14.9/457.1 MB 13.1 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 15.8/457.1 MB 13.4 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 16.5/457.1 MB 13.6 MB/s eta 0:00:33\n",
      "     - ------------------------------------ 17.4/457.1 MB 13.9 MB/s eta 0:00:32\n",
      "     - ------------------------------------ 18.2/457.1 MB 14.6 MB/s eta 0:00:31\n",
      "     - ------------------------------------ 19.0/457.1 MB 14.9 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 19.8/457.1 MB 15.2 MB/s eta 0:00:29\n",
      "     - ------------------------------------ 20.4/457.1 MB 14.9 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 21.1/457.1 MB 14.9 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 22.0/457.1 MB 14.9 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 22.8/457.1 MB 14.9 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 23.3/457.1 MB 14.6 MB/s eta 0:00:30\n",
      "     - ------------------------------------ 23.8/457.1 MB 14.2 MB/s eta 0:00:31\n",
      "     -- ----------------------------------- 24.7/457.1 MB 14.2 MB/s eta 0:00:31\n",
      "     -- ----------------------------------- 25.2/457.1 MB 13.6 MB/s eta 0:00:32\n",
      "     -- ----------------------------------- 26.0/457.1 MB 13.6 MB/s eta 0:00:32\n",
      "     -- ----------------------------------- 26.7/457.1 MB 13.4 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 27.2/457.1 MB 13.4 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 27.9/457.1 MB 13.6 MB/s eta 0:00:32\n",
      "     -- ----------------------------------- 28.7/457.1 MB 13.4 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 29.2/457.1 MB 13.1 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 30.2/457.1 MB 13.1 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 31.1/457.1 MB 13.4 MB/s eta 0:00:32\n",
      "     -- ----------------------------------- 31.9/457.1 MB 13.1 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 32.9/457.1 MB 13.6 MB/s eta 0:00:32\n",
      "     -- ----------------------------------- 33.7/457.1 MB 14.6 MB/s eta 0:00:30\n",
      "     -- ----------------------------------- 34.4/457.1 MB 14.6 MB/s eta 0:00:30\n",
      "     -- ----------------------------------- 35.0/457.1 MB 14.6 MB/s eta 0:00:30\n",
      "     -- ----------------------------------- 35.6/457.1 MB 15.2 MB/s eta 0:00:28\n",
      "     --- ---------------------------------- 36.5/457.1 MB 14.9 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 37.2/457.1 MB 15.2 MB/s eta 0:00:28\n",
      "     --- ---------------------------------- 37.9/457.1 MB 15.6 MB/s eta 0:00:27\n",
      "     --- ---------------------------------- 38.5/457.1 MB 14.9 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 39.5/457.1 MB 15.6 MB/s eta 0:00:27\n",
      "     --- ---------------------------------- 40.3/457.1 MB 15.2 MB/s eta 0:00:28\n",
      "     --- ---------------------------------- 41.5/457.1 MB 16.0 MB/s eta 0:00:27\n",
      "     --- ---------------------------------- 41.8/457.1 MB 15.6 MB/s eta 0:00:27\n",
      "     --- ---------------------------------- 42.8/457.1 MB 15.6 MB/s eta 0:00:27\n",
      "     --- ---------------------------------- 43.3/457.1 MB 15.2 MB/s eta 0:00:28\n",
      "     --- ---------------------------------- 43.9/457.1 MB 14.2 MB/s eta 0:00:30\n",
      "     --- ---------------------------------- 44.5/457.1 MB 14.2 MB/s eta 0:00:30\n",
      "     --- ---------------------------------- 45.2/457.1 MB 14.6 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 45.7/457.1 MB 13.9 MB/s eta 0:00:30\n",
      "     --- ---------------------------------- 46.7/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 47.4/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 48.4/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 48.9/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 49.3/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 50.2/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 50.8/457.1 MB 13.9 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 51.4/457.1 MB 13.1 MB/s eta 0:00:32\n",
      "     ---- --------------------------------- 52.7/457.1 MB 13.9 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 53.4/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 53.9/457.1 MB 13.9 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 54.5/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 55.1/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 56.1/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 56.8/457.1 MB 13.9 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 57.7/457.1 MB 13.9 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 58.3/457.1 MB 14.2 MB/s eta 0:00:29\n",
      "     ---- --------------------------------- 58.9/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ---- --------------------------------- 59.6/457.1 MB 14.2 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 60.2/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ----- -------------------------------- 60.9/457.1 MB 13.6 MB/s eta 0:00:30\n",
      "     ----- -------------------------------- 61.9/457.1 MB 14.9 MB/s eta 0:00:27\n",
      "     ----- -------------------------------- 62.5/457.1 MB 14.2 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 63.4/457.1 MB 14.2 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 63.9/457.1 MB 14.6 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 64.3/457.1 MB 13.6 MB/s eta 0:00:29\n",
      "     ----- -------------------------------- 65.4/457.1 MB 15.2 MB/s eta 0:00:26\n",
      "     ----- -------------------------------- 66.1/457.1 MB 14.9 MB/s eta 0:00:27\n",
      "     ----- -------------------------------- 66.3/457.1 MB 14.2 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 66.9/457.1 MB 13.9 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 67.8/457.1 MB 13.9 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 68.7/457.1 MB 13.6 MB/s eta 0:00:29\n",
      "     ----- -------------------------------- 69.1/457.1 MB 13.9 MB/s eta 0:00:28\n",
      "     ----- -------------------------------- 69.5/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ----- -------------------------------- 70.1/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ----- -------------------------------- 71.0/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ----- -------------------------------- 71.7/457.1 MB 13.1 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 72.3/457.1 MB 12.8 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 73.0/457.1 MB 13.1 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 73.6/457.1 MB 12.8 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 74.3/457.1 MB 13.1 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 75.0/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 75.6/457.1 MB 12.8 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 76.6/457.1 MB 13.6 MB/s eta 0:00:28\n",
      "     ------ ------------------------------- 77.3/457.1 MB 13.6 MB/s eta 0:00:28\n",
      "     ------ ------------------------------- 78.0/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 78.7/457.1 MB 13.1 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 79.3/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 79.8/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 80.3/457.1 MB 13.1 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 80.8/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 81.7/457.1 MB 13.1 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 82.7/457.1 MB 13.6 MB/s eta 0:00:28\n",
      "     ------ ------------------------------- 83.1/457.1 MB 13.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 83.9/457.1 MB 13.4 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 84.4/457.1 MB 13.6 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 85.2/457.1 MB 13.4 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 85.7/457.1 MB 13.1 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 86.5/457.1 MB 13.4 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 87.2/457.1 MB 13.6 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 87.9/457.1 MB 13.9 MB/s eta 0:00:27\n",
      "     ------- ------------------------------ 88.6/457.1 MB 14.6 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 89.2/457.1 MB 14.2 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 90.0/457.1 MB 14.2 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 90.8/457.1 MB 14.6 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 91.8/457.1 MB 14.5 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 92.9/457.1 MB 14.6 MB/s eta 0:00:26\n",
      "     ------- ------------------------------ 93.4/457.1 MB 14.9 MB/s eta 0:00:25\n",
      "     ------- ------------------------------ 94.2/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     ------- ------------------------------ 94.9/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     ------- ------------------------------ 95.8/457.1 MB 14.9 MB/s eta 0:00:25\n",
      "     -------- ----------------------------- 96.3/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ----------------------------- 97.2/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ----------------------------- 97.6/457.1 MB 14.2 MB/s eta 0:00:26\n",
      "     -------- ----------------------------- 98.6/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ----------------------------- 99.3/457.1 MB 14.5 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 100.1/457.1 MB 15.2 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 100.7/457.1 MB 15.2 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 101.3/457.1 MB 15.2 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 102.0/457.1 MB 14.9 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 103.0/457.1 MB 14.2 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 103.6/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 104.4/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 105.4/457.1 MB 14.9 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 105.9/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 106.5/457.1 MB 14.6 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 107.4/457.1 MB 14.9 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 108.2/457.1 MB 14.5 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 109.4/457.1 MB 14.9 MB/s eta 0:00:24\n",
      "     -------- ---------------------------- 110.0/457.1 MB 14.2 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 110.5/457.1 MB 14.2 MB/s eta 0:00:25\n",
      "     -------- ---------------------------- 111.1/457.1 MB 14.2 MB/s eta 0:00:25\n",
      "     --------- --------------------------- 112.4/457.1 MB 15.2 MB/s eta 0:00:23\n",
      "     --------- --------------------------- 113.0/457.1 MB 15.2 MB/s eta 0:00:23\n",
      "     --------- --------------------------- 113.9/457.1 MB 15.2 MB/s eta 0:00:23\n",
      "     --------- --------------------------- 114.7/457.1 MB 15.6 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 115.5/457.1 MB 15.6 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 116.0/457.1 MB 15.6 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 116.9/457.1 MB 16.0 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 117.5/457.1 MB 15.2 MB/s eta 0:00:23\n",
      "     --------- --------------------------- 118.8/457.1 MB 16.0 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 119.4/457.1 MB 15.6 MB/s eta 0:00:22\n",
      "     --------- --------------------------- 120.4/457.1 MB 16.4 MB/s eta 0:00:21\n",
      "     --------- --------------------------- 121.1/457.1 MB 17.3 MB/s eta 0:00:20\n",
      "     --------- --------------------------- 121.7/457.1 MB 16.8 MB/s eta 0:00:20\n",
      "     --------- --------------------------- 122.6/457.1 MB 16.0 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 123.8/457.1 MB 16.8 MB/s eta 0:00:20\n",
      "     ---------- -------------------------- 124.5/457.1 MB 16.4 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 125.1/457.1 MB 16.4 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 125.8/457.1 MB 16.0 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 126.4/457.1 MB 16.0 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 127.2/457.1 MB 16.0 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 128.0/457.1 MB 16.8 MB/s eta 0:00:20\n",
      "     ---------- -------------------------- 128.6/457.1 MB 16.4 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 129.2/457.1 MB 15.2 MB/s eta 0:00:22\n",
      "     ---------- -------------------------- 130.1/457.1 MB 15.6 MB/s eta 0:00:22\n",
      "     ---------- -------------------------- 130.7/457.1 MB 15.2 MB/s eta 0:00:22\n",
      "     ---------- -------------------------- 131.5/457.1 MB 15.2 MB/s eta 0:00:22\n",
      "     ---------- -------------------------- 132.7/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 133.7/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 134.2/457.1 MB 15.2 MB/s eta 0:00:22\n",
      "     ---------- -------------------------- 135.1/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ---------- -------------------------- 135.9/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 136.5/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 137.2/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 138.3/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 139.1/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 139.9/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 140.4/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 140.9/457.1 MB 14.5 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 141.9/457.1 MB 15.6 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 142.8/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 143.7/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 144.1/457.1 MB 14.9 MB/s eta 0:00:22\n",
      "     ----------- ------------------------- 144.9/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 145.6/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 146.4/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 147.1/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 147.5/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ----------- ------------------------- 148.0/457.1 MB 14.6 MB/s eta 0:00:22\n",
      "     ------------ ------------------------ 149.1/457.1 MB 13.9 MB/s eta 0:00:23\n",
      "     ------------ ------------------------ 149.9/457.1 MB 14.2 MB/s eta 0:00:22\n",
      "     ------------ ------------------------ 150.6/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 151.4/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 152.2/457.1 MB 15.2 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 152.9/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 153.7/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 154.2/457.1 MB 14.5 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 154.9/457.1 MB 14.6 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 155.8/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 156.5/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 156.9/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 157.8/457.1 MB 15.6 MB/s eta 0:00:20\n",
      "     ------------ ------------------------ 158.1/457.1 MB 15.2 MB/s eta 0:00:20\n",
      "     ------------ ------------------------ 158.6/457.1 MB 14.9 MB/s eta 0:00:21\n",
      "     ------------ ------------------------ 159.4/457.1 MB 14.6 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 160.7/457.1 MB 14.9 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 161.1/457.1 MB 14.6 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 161.8/457.1 MB 14.2 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 162.9/457.1 MB 14.6 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 163.2/457.1 MB 13.9 MB/s eta 0:00:22\n",
      "     ------------- ----------------------- 164.2/457.1 MB 13.6 MB/s eta 0:00:22\n",
      "     ------------- ----------------------- 164.8/457.1 MB 13.6 MB/s eta 0:00:22\n",
      "     ------------- ----------------------- 165.5/457.1 MB 13.4 MB/s eta 0:00:22\n",
      "     ------------- ----------------------- 166.5/457.1 MB 13.9 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 167.1/457.1 MB 14.6 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 167.9/457.1 MB 14.2 MB/s eta 0:00:21\n",
      "     ------------- ----------------------- 168.7/457.1 MB 14.9 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 169.4/457.1 MB 15.2 MB/s eta 0:00:19\n",
      "     ------------- ----------------------- 170.4/457.1 MB 14.9 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 171.2/457.1 MB 14.9 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 171.7/457.1 MB 14.9 MB/s eta 0:00:20\n",
      "     ------------- ----------------------- 172.5/457.1 MB 15.2 MB/s eta 0:00:19\n",
      "     -------------- ---------------------- 173.1/457.1 MB 14.6 MB/s eta 0:00:20\n",
      "     -------------- ---------------------- 174.0/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 174.6/457.1 MB 15.6 MB/s eta 0:00:19\n",
      "     -------------- ---------------------- 175.5/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 176.3/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 177.0/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 177.8/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 178.2/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 179.2/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 179.6/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 180.6/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 181.1/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 182.1/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 182.6/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 183.4/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 184.3/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     -------------- ---------------------- 184.8/457.1 MB 16.0 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 185.7/457.1 MB 15.6 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 185.9/457.1 MB 14.9 MB/s eta 0:00:19\n",
      "     --------------- --------------------- 187.0/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 187.6/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 188.5/457.1 MB 16.0 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 188.8/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 189.3/457.1 MB 14.5 MB/s eta 0:00:19\n",
      "     --------------- --------------------- 190.4/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 190.9/457.1 MB 14.6 MB/s eta 0:00:19\n",
      "     --------------- --------------------- 191.7/457.1 MB 14.6 MB/s eta 0:00:19\n",
      "     --------------- --------------------- 192.8/457.1 MB 14.6 MB/s eta 0:00:19\n",
      "     --------------- --------------------- 193.5/457.1 MB 14.9 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 194.8/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     --------------- --------------------- 195.7/457.1 MB 15.6 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 196.1/457.1 MB 15.6 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 196.4/457.1 MB 14.9 MB/s eta 0:00:18\n",
      "     ---------------- -------------------- 197.7/457.1 MB 15.2 MB/s eta 0:00:18\n",
      "     ---------------- -------------------- 198.4/457.1 MB 14.9 MB/s eta 0:00:18\n",
      "     ---------------- -------------------- 199.6/457.1 MB 16.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 199.9/457.1 MB 15.6 MB/s eta 0:00:17\n",
      "     ---------------- -------------------- 200.6/457.1 MB 14.9 MB/s eta 0:00:18\n",
      "     ---------------- -------------------- 201.9/457.1 MB 16.8 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 202.9/457.1 MB 16.8 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 203.7/457.1 MB 16.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 204.4/457.1 MB 16.0 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 205.7/457.1 MB 15.6 MB/s eta 0:00:17\n",
      "     ---------------- -------------------- 206.3/457.1 MB 16.0 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 207.0/457.1 MB 16.8 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 208.1/457.1 MB 16.8 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 208.4/457.1 MB 16.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 209.3/457.1 MB 16.4 MB/s eta 0:00:16\n",
      "     ---------------- -------------------- 209.9/457.1 MB 16.4 MB/s eta 0:00:16\n",
      "     ----------------- ------------------- 211.3/457.1 MB 17.7 MB/s eta 0:00:14\n",
      "     ----------------- ------------------- 212.3/457.1 MB 17.7 MB/s eta 0:00:14\n",
      "     ----------------- ------------------- 212.9/457.1 MB 16.8 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 213.9/457.1 MB 16.0 MB/s eta 0:00:16\n",
      "     ----------------- ------------------- 215.0/457.1 MB 17.2 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 215.6/457.1 MB 16.4 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 216.8/457.1 MB 17.7 MB/s eta 0:00:14\n",
      "     ----------------- ------------------- 217.3/457.1 MB 17.3 MB/s eta 0:00:14\n",
      "     ----------------- ------------------- 217.6/457.1 MB 16.8 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 218.3/457.1 MB 15.6 MB/s eta 0:00:16\n",
      "     ----------------- ------------------- 219.6/457.1 MB 16.4 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 220.4/457.1 MB 17.2 MB/s eta 0:00:14\n",
      "     ----------------- ------------------- 221.6/457.1 MB 16.4 MB/s eta 0:00:15\n",
      "     ----------------- ------------------- 222.0/457.1 MB 15.6 MB/s eta 0:00:16\n",
      "     ------------------ ------------------ 222.7/457.1 MB 15.6 MB/s eta 0:00:16\n",
      "     ------------------ ------------------ 223.4/457.1 MB 16.0 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 224.2/457.1 MB 16.4 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 225.1/457.1 MB 16.0 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 225.7/457.1 MB 16.0 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 226.3/457.1 MB 15.2 MB/s eta 0:00:16\n",
      "     ------------------ ------------------ 227.7/457.1 MB 16.4 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 228.8/457.1 MB 17.7 MB/s eta 0:00:13\n",
      "     ------------------ ------------------ 229.3/457.1 MB 16.8 MB/s eta 0:00:14\n",
      "     ------------------ ------------------ 230.1/457.1 MB 16.0 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 230.8/457.1 MB 16.0 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 231.7/457.1 MB 15.2 MB/s eta 0:00:15\n",
      "     ------------------ ------------------ 232.9/457.1 MB 16.4 MB/s eta 0:00:14\n",
      "     ------------------ ------------------ 233.8/457.1 MB 16.8 MB/s eta 0:00:14\n",
      "     ------------------ ------------------ 234.7/457.1 MB 16.8 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 235.4/457.1 MB 16.4 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 236.7/457.1 MB 17.7 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 237.3/457.1 MB 16.8 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 238.3/457.1 MB 17.2 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 238.7/457.1 MB 16.4 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 239.6/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 241.0/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 241.5/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 242.4/457.1 MB 17.2 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 243.4/457.1 MB 16.4 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 244.5/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 244.8/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     ------------------- ----------------- 245.7/457.1 MB 16.0 MB/s eta 0:00:14\n",
      "     ------------------- ----------------- 246.9/457.1 MB 16.0 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 248.0/457.1 MB 16.0 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 248.6/457.1 MB 15.2 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 249.9/457.1 MB 16.8 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 250.3/457.1 MB 16.4 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 251.2/457.1 MB 15.6 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 252.1/457.1 MB 16.0 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 253.0/457.1 MB 15.6 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 253.6/457.1 MB 15.2 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 254.3/457.1 MB 15.2 MB/s eta 0:00:14\n",
      "     -------------------- ---------------- 255.4/457.1 MB 16.4 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 256.0/457.1 MB 16.4 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 256.8/457.1 MB 16.0 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 257.4/457.1 MB 15.6 MB/s eta 0:00:13\n",
      "     -------------------- ---------------- 258.9/457.1 MB 16.8 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 259.8/457.1 MB 16.4 MB/s eta 0:00:13\n",
      "     --------------------- --------------- 260.3/457.1 MB 16.0 MB/s eta 0:00:13\n",
      "     --------------------- --------------- 261.0/457.1 MB 16.0 MB/s eta 0:00:13\n",
      "     --------------------- --------------- 261.8/457.1 MB 16.4 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 262.7/457.1 MB 16.8 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 263.9/457.1 MB 16.8 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 265.0/457.1 MB 17.2 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 266.0/457.1 MB 17.3 MB/s eta 0:00:12\n",
      "     --------------------- --------------- 267.1/457.1 MB 18.2 MB/s eta 0:00:11\n",
      "     --------------------- --------------- 267.8/457.1 MB 18.7 MB/s eta 0:00:11\n",
      "     --------------------- --------------- 268.7/457.1 MB 17.7 MB/s eta 0:00:11\n",
      "     --------------------- --------------- 269.7/457.1 MB 17.3 MB/s eta 0:00:11\n",
      "     --------------------- --------------- 271.0/457.1 MB 18.2 MB/s eta 0:00:11\n",
      "     --------------------- --------------- 271.7/457.1 MB 18.2 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 272.7/457.1 MB 18.7 MB/s eta 0:00:10\n",
      "     ---------------------- -------------- 273.4/457.1 MB 18.7 MB/s eta 0:00:10\n",
      "     ---------------------- -------------- 273.9/457.1 MB 17.7 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 274.9/457.1 MB 17.7 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 275.8/457.1 MB 17.2 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 276.7/457.1 MB 17.2 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 277.2/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 277.9/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 278.7/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 279.5/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 280.3/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 281.3/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 282.1/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 282.5/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ---------------------- -------------- 283.4/457.1 MB 15.6 MB/s eta 0:00:12\n",
      "     ----------------------- ------------- 284.8/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 285.4/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 285.9/457.1 MB 16.0 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 287.0/457.1 MB 16.0 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 288.4/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 289.1/457.1 MB 16.8 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 290.3/457.1 MB 16.8 MB/s eta 0:00:10\n",
      "     ----------------------- ------------- 290.9/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 291.8/457.1 MB 16.0 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 292.6/457.1 MB 16.4 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 293.4/457.1 MB 17.7 MB/s eta 0:00:10\n",
      "     ----------------------- ------------- 293.9/457.1 MB 16.4 MB/s eta 0:00:10\n",
      "     ----------------------- ------------- 294.6/457.1 MB 15.6 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 295.3/457.1 MB 15.2 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 295.6/457.1 MB 15.2 MB/s eta 0:00:11\n",
      "     ----------------------- ------------- 296.2/457.1 MB 15.2 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 297.4/457.1 MB 15.6 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 297.6/457.1 MB 14.9 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 298.2/457.1 MB 14.2 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 299.3/457.1 MB 14.2 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 300.1/457.1 MB 14.2 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 300.6/457.1 MB 13.6 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 301.3/457.1 MB 13.6 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 302.0/457.1 MB 13.4 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 303.1/457.1 MB 13.6 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 303.6/457.1 MB 13.1 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 304.6/457.1 MB 13.4 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 305.0/457.1 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 305.8/457.1 MB 13.6 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 306.7/457.1 MB 14.6 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 307.7/457.1 MB 14.2 MB/s eta 0:00:11\n",
      "     ------------------------ ------------ 308.6/457.1 MB 15.6 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 309.1/457.1 MB 14.9 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 310.2/457.1 MB 14.2 MB/s eta 0:00:11\n",
      "     ------------------------- ----------- 310.8/457.1 MB 14.9 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 311.3/457.1 MB 14.6 MB/s eta 0:00:11\n",
      "     ------------------------- ----------- 312.0/457.1 MB 14.6 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 312.6/457.1 MB 14.9 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 313.6/457.1 MB 14.6 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 314.2/457.1 MB 14.5 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 314.8/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 315.6/457.1 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------------------- ----------- 316.6/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 317.8/457.1 MB 14.6 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 318.5/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 319.5/457.1 MB 14.9 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 319.8/457.1 MB 13.9 MB/s eta 0:00:10\n",
      "     ------------------------- ----------- 320.7/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 321.5/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 322.4/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 322.7/457.1 MB 14.6 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 323.3/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 324.1/457.1 MB 14.6 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 325.0/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 325.5/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 325.9/457.1 MB 14.9 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 326.8/457.1 MB 14.5 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 327.6/457.1 MB 14.2 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 329.2/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 330.2/457.1 MB 16.0 MB/s eta 0:00:08\n",
      "     -------------------------- ---------- 330.9/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 331.9/457.1 MB 15.2 MB/s eta 0:00:09\n",
      "     -------------------------- ---------- 333.1/457.1 MB 16.4 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 333.9/457.1 MB 16.8 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 334.2/457.1 MB 16.0 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 334.9/457.1 MB 15.6 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 335.5/457.1 MB 15.6 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 336.0/457.1 MB 15.6 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 337.6/457.1 MB 16.8 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 337.7/457.1 MB 16.4 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 337.7/457.1 MB 16.4 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 337.7/457.1 MB 16.4 MB/s eta 0:00:08\n",
      "     --------------------------- --------- 338.4/457.1 MB 13.1 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 340.0/457.1 MB 13.9 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 340.8/457.1 MB 13.9 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 341.5/457.1 MB 14.2 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 342.1/457.1 MB 13.6 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 342.3/457.1 MB 12.8 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 342.3/457.1 MB 12.8 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 343.1/457.1 MB 11.5 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 344.3/457.1 MB 11.9 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 344.7/457.1 MB 11.9 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 345.3/457.1 MB 11.7 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 345.8/457.1 MB 11.9 MB/s eta 0:00:10\n",
      "     ---------------------------- -------- 346.5/457.1 MB 12.1 MB/s eta 0:00:10\n",
      "     ---------------------------- -------- 347.4/457.1 MB 11.9 MB/s eta 0:00:10\n",
      "     ---------------------------- -------- 348.1/457.1 MB 14.2 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 349.3/457.1 MB 14.2 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 350.1/457.1 MB 13.6 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 350.9/457.1 MB 13.6 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 351.5/457.1 MB 13.6 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 351.9/457.1 MB 13.1 MB/s eta 0:00:09\n",
      "     ---------------------------- -------- 353.2/457.1 MB 16.0 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 354.1/457.1 MB 16.0 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 354.4/457.1 MB 14.9 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 355.1/457.1 MB 14.9 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 355.6/457.1 MB 15.2 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 356.9/457.1 MB 15.6 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 357.8/457.1 MB 15.6 MB/s eta 0:00:07\n",
      "     ---------------------------- -------- 358.2/457.1 MB 14.9 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 359.8/457.1 MB 15.6 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 360.0/457.1 MB 14.9 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 360.3/457.1 MB 13.9 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 361.1/457.1 MB 13.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 361.4/457.1 MB 12.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 362.3/457.1 MB 13.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 363.5/457.1 MB 13.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 363.7/457.1 MB 12.6 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 364.2/457.1 MB 12.1 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 364.9/457.1 MB 12.9 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 366.0/457.1 MB 13.1 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 367.6/457.1 MB 13.1 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 368.5/457.1 MB 14.2 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 369.3/457.1 MB 13.6 MB/s eta 0:00:07\n",
      "     ----------------------------- ------- 370.0/457.1 MB 12.8 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 370.8/457.1 MB 14.2 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 371.8/457.1 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 372.3/457.1 MB 15.2 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 373.2/457.1 MB 14.9 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 374.1/457.1 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 374.6/457.1 MB 16.0 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 375.4/457.1 MB 16.4 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 376.3/457.1 MB 16.4 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 377.3/457.1 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 378.1/457.1 MB 15.6 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 379.1/457.1 MB 15.2 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 379.8/457.1 MB 15.2 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 380.3/457.1 MB 15.2 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 381.1/457.1 MB 16.0 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 381.7/457.1 MB 15.2 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 382.3/457.1 MB 15.2 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 382.9/457.1 MB 15.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 383.4/457.1 MB 14.5 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 384.4/457.1 MB 14.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 385.2/457.1 MB 14.9 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 386.0/457.1 MB 14.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 386.5/457.1 MB 14.2 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 387.3/457.1 MB 13.9 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 388.4/457.1 MB 13.9 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 389.3/457.1 MB 13.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 390.6/457.1 MB 14.2 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 391.4/457.1 MB 14.9 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 391.8/457.1 MB 14.2 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 392.4/457.1 MB 14.2 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 393.6/457.1 MB 14.9 MB/s eta 0:00:05\n",
      "     ------------------------------- ----- 394.7/457.1 MB 15.2 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 396.0/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 396.4/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 397.0/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 397.7/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 399.0/457.1 MB 16.4 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 399.5/457.1 MB 16.0 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 399.9/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 400.4/457.1 MB 14.9 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 401.3/457.1 MB 14.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 402.4/457.1 MB 15.2 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 403.1/457.1 MB 15.2 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 404.6/457.1 MB 16.0 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 405.1/457.1 MB 16.0 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 405.8/457.1 MB 15.2 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 407.2/457.1 MB 16.0 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 407.9/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 408.5/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 409.3/457.1 MB 15.6 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 410.4/457.1 MB 16.4 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 411.1/457.1 MB 17.7 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 411.5/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 412.1/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 413.2/457.1 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 413.7/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 414.6/457.1 MB 15.6 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 415.5/457.1 MB 16.0 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 415.9/457.1 MB 15.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 417.3/457.1 MB 15.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 418.4/457.1 MB 16.0 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 419.5/457.1 MB 16.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 420.5/457.1 MB 16.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 421.6/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 422.6/457.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 423.4/457.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 424.3/457.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 425.2/457.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 426.0/457.1 MB 18.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 427.0/457.1 MB 18.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 427.9/457.1 MB 18.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 428.6/457.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 429.5/457.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 430.5/457.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 430.9/457.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 432.2/457.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 432.5/457.1 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 433.3/457.1 MB 15.6 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 433.9/457.1 MB 15.6 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 435.4/457.1 MB 16.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 436.5/457.1 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 437.3/457.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 438.0/457.1 MB 16.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 438.4/457.1 MB 15.6 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 439.0/457.1 MB 15.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 439.8/457.1 MB 15.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 440.7/457.1 MB 14.9 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 441.8/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 442.4/457.1 MB 14.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 442.9/457.1 MB 15.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 443.7/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  444.8/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  445.6/457.1 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  446.8/457.1 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.2/457.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.7/457.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  448.4/457.1 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  449.4/457.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  450.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  450.8/457.1 MB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.8/457.1 MB 15.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  452.3/457.1 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  453.1/457.1 MB 14.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  454.0/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  455.1/457.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  456.1/457.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------------  456.9/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.1/457.1 MB 15.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 457.1/457.1 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-curated-transformers<0.3.0,>=0.2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from ca-core-news-trf==3.7.2) (0.2.2)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from ca-core-news-trf==3.7.2) (3.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (4.66.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (8.2.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.26.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (65.5.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.1.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.7.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (24.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.32.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.4.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (0.0.9)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (0.1.1)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2.3.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2024.5.15)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (4.12.1)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.18.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (3.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: sympy in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (1.12.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (3.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ca-core-news-trf==3.7.2) (1.1.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (2021.12.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->ca-core-news-trf==3.7.2) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ca_core_news_trf')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ca_core_news_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model de RoBERTa preentrenat en català."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_model = spacy.load(\"ca_core_news_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_roberta()*. Aquesta funció utilitza el model de RoBERTa per a convertir les frases a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_roberta_train = map_roberta_embed(train, roberta_model)\n",
    "mapped_roberta_test = map_roberta_embed(test, roberta_model)\n",
    "mapped_roberta_val = map_roberta_embed(val, roberta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividim les particions en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_roberta, y_train_roberta = pair_list_to_x_y(mapped_roberta_train)\n",
    "x_val_roberta, y_val_roberta = pair_list_to_x_y(mapped_roberta_val)\n",
    "x_test_roberta, y_test_roberta = pair_list_to_x_y(mapped_roberta_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_roberta: int = 64\n",
    "num_epochs_roberta: int = 64\n",
    "\n",
    "train_dataset_roberta = tf.data.Dataset.from_tensor_slices((x_train_roberta, y_train_roberta))\n",
    "train_dataset_roberta = train_dataset_roberta.shuffle(buffer_size=len(x_train_roberta)).batch(batch_size_roberta)\n",
    "\n",
    "val_dataset_roberta = tf.data.Dataset.from_tensor_slices((x_val_roberta, y_val_roberta))\n",
    "val_dataset_roberta = val_dataset_roberta.batch(batch_size_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │    \u001b[38;5;34m590,592\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ dense[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │     \u001b[38;5;34m12,304\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">602,913</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m602,913\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">602,913</span> (2.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m602,913\u001b[0m (2.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size_roberta = 768\n",
    "model_roberta = build_and_compile_model_better(embedding_size = embedding_size_roberta)\n",
    "tf.keras.utils.plot_model(model_roberta, show_shapes=True, show_layer_activations=True)\n",
    "print(model_roberta.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.7264 - val_loss: 0.7412\n",
      "Epoch 2/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7196 - val_loss: 0.7335\n",
      "Epoch 3/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.7089 - val_loss: 0.7231\n",
      "Epoch 4/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6908 - val_loss: 0.7103\n",
      "Epoch 5/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6668 - val_loss: 0.7003\n",
      "Epoch 6/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6353 - val_loss: 0.6926\n",
      "Epoch 7/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6061 - val_loss: 0.6760\n",
      "Epoch 8/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5609 - val_loss: 0.6610\n",
      "Epoch 9/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.5258 - val_loss: 0.6500\n",
      "Epoch 10/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5042 - val_loss: 0.6386\n",
      "Epoch 11/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4783 - val_loss: 0.6312\n",
      "Epoch 12/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4420 - val_loss: 0.6148\n",
      "Epoch 13/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4239 - val_loss: 0.6043\n",
      "Epoch 14/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4039 - val_loss: 0.6028\n",
      "Epoch 15/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3737 - val_loss: 0.6068\n",
      "Epoch 16/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3637 - val_loss: 0.5913\n",
      "Epoch 17/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3350 - val_loss: 0.5941\n",
      "Epoch 18/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3334 - val_loss: 0.6399\n",
      "Epoch 19/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3346 - val_loss: 0.5940\n",
      "Epoch 20/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2886 - val_loss: 0.5991\n",
      "Epoch 21/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2671 - val_loss: 0.5935\n",
      "Epoch 22/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.2561 - val_loss: 0.5766\n",
      "Epoch 23/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2405 - val_loss: 0.5839\n",
      "Epoch 24/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2279 - val_loss: 0.5934\n",
      "Epoch 25/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2189 - val_loss: 0.5952\n",
      "Epoch 26/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2111 - val_loss: 0.5960\n",
      "Epoch 27/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2040 - val_loss: 0.6017\n",
      "Epoch 28/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1966 - val_loss: 0.5901\n",
      "Epoch 29/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1812 - val_loss: 0.5912\n",
      "Epoch 30/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1731 - val_loss: 0.6037\n",
      "Epoch 31/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1721 - val_loss: 0.6092\n",
      "Epoch 32/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1717 - val_loss: 0.6140\n",
      "Epoch 33/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1663 - val_loss: 0.6153\n",
      "Epoch 34/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1574 - val_loss: 0.6392\n",
      "Epoch 35/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1474 - val_loss: 0.6328\n",
      "Epoch 36/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1505 - val_loss: 0.6410\n",
      "Epoch 37/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1442 - val_loss: 0.6412\n",
      "Epoch 38/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1493 - val_loss: 0.6858\n",
      "Epoch 39/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1459 - val_loss: 0.7054\n",
      "Epoch 40/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1420 - val_loss: 0.7078\n",
      "Epoch 41/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1456 - val_loss: 0.6385\n",
      "Epoch 42/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1314 - val_loss: 0.7035\n",
      "Epoch 43/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1665 - val_loss: 0.7157\n",
      "Epoch 44/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1508 - val_loss: 0.6986\n",
      "Epoch 45/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1556 - val_loss: 0.6920\n",
      "Epoch 46/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1625 - val_loss: 0.6726\n",
      "Epoch 47/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1495 - val_loss: 0.6083\n",
      "Epoch 48/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1370 - val_loss: 0.6412\n",
      "Epoch 49/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1223 - val_loss: 0.6605\n",
      "Epoch 50/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1085 - val_loss: 0.6785\n",
      "Epoch 51/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1081 - val_loss: 0.7016\n",
      "Epoch 52/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1064 - val_loss: 0.7204\n",
      "Epoch 53/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1270 - val_loss: 0.7212\n",
      "Epoch 54/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1130 - val_loss: 0.6992\n",
      "Epoch 55/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1056 - val_loss: 0.6595\n",
      "Epoch 56/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1082 - val_loss: 0.6634\n",
      "Epoch 57/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1067 - val_loss: 0.6226\n",
      "Epoch 58/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1144 - val_loss: 0.6535\n",
      "Epoch 59/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1174 - val_loss: 0.6256\n",
      "Epoch 60/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1139 - val_loss: 0.6381\n",
      "Epoch 61/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1123 - val_loss: 0.6582\n",
      "Epoch 62/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1106 - val_loss: 0.6465\n",
      "Epoch 63/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0959 - val_loss: 0.6297\n",
      "Epoch 64/64\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1017 - val_loss: 0.6596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c951955a50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_roberta.fit(train_dataset_roberta, epochs=num_epochs_roberta, validation_data=val_dataset_roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el model amb la partició de validació."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Correlación de Pearson (train): 0.9432079034985339\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Correlación de Pearson (validation): 0.4552241655201091\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_roberta, x_train_roberta, y_train_roberta)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_roberta, x_val_roberta, y_val_roberta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provar el model amb la partició de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Correlación de Pearson (test): 0.3001994328819157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_roberta, x_test_roberta, y_test_roberta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem com el model de RoBERTa és un dels que millor funciona, ja que és el model més avançat i el que millor semàntica té. Això fa que el model sigui capaç de generalitzar millor.\n",
    "Per altra banda, els seus vectors són més grans així que és més pesat d'executar que els altres models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instal·lem les llibreries necessàries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.7 MB 3.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.3/1.7 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.7 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.8/1.7 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.2/1.7 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.6/1.7 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.7/1.7 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.64.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.0)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: optree in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: namex in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pol\\desktop\\1_uni\\q4\\plh\\plh-wordembeddings\\plh-we-env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fem els imports de les funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'compute_pearson_roberta_ft' from 'roberta_fine_tuned' (c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\roberta_fine_tuned.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroberta_fine_tuned\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_roberta_ft, compute_pearson_roberta_ft, x_y_split_roberta_ft\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'compute_pearson_roberta_ft' from 'roberta_fine_tuned' (c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\roberta_fine_tuned.py)"
     ]
    }
   ],
   "source": [
    "from roberta_fine_tuned import prepare_roberta_ft, compute_pearson_roberta_ft, x_y_split_roberta_ft\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem el model de RoBERTa fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pol\\.cache\\huggingface\\hub\\models--projecte-aina--roberta-base-ca-v2-cased-sts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Pol\\Desktop\\1_Uni\\Q4\\PLH\\PLH-WordEmbeddings\\PLH-WE-env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_roberta_ft = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer_roberta_ft = AutoTokenizer.from_pretrained(model_roberta_ft)\n",
    "pipe_roberta_ft = pipeline('text-classification', model=model_roberta_ft, tokenizer=tokenizer_roberta_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem en X i Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_roberta_ft, y_train_roberta_ft = x_y_split_roberta_ft(train)\n",
    "X_val_roberta_ft, y_val_roberta_ft = x_y_split_roberta_ft(val)\n",
    "X_test_roberta_ft, y_test_roberta_ft = x_y_split_roberta_ft(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_roberta_ft + X_val_roberta_ft + X_test_roberta_ft\n",
    "y = y_train_roberta_ft + y_val_roberta_ft + y_test_roberta_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenim els resultats del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe_roberta_ft(prepare_roberta_ft(X, tokenizer_roberta_ft), add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación de Pearson (all data): 0.8905615573801516\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (all data): {compute_pearson_roberta_ft(predictions, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlación de Pearson (test): 0.7527686781420264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson_roberta_ft(predictions[-len(y_test_roberta_ft):], y_test_roberta_ft)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions Roberta fine-tuned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordEmbeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
