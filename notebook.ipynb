{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLH - Rubén Álvarez Aragonés i Pol Pérez Prades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.ca.examples import sentences \n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Requisites\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "spacy.cli.download(\"ca_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedder:\n",
    "    def __init__(self, corpus_path, corpus_size, load_model=False, model_path=None):\n",
    "        if not load_model:\n",
    "            self.corpus_path = corpus_path\n",
    "            self.corpus_size = int(corpus_size * 2**30) if corpus_size else None  # Convert GB to bytes\n",
    "            self.corpus = self.get_corpus(corpus_path)\n",
    "            self.fit()\n",
    "        else:\n",
    "            try:\n",
    "                self.load(model_path)\n",
    "            except FileNotFoundError:\n",
    "                print(\"Model not found. Please check the path.\")\n",
    "                return\n",
    "\n",
    "    def get_corpus(self, corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            print(\"Reading corpus...\")\n",
    "            corpus = f.read(self.corpus_size) if self.corpus_size else f.read()\n",
    "            print(\"Preprocessing corpus...\")\n",
    "            corpus = self.preprocess(corpus)  # Preprocess the corpus and tokenize it\n",
    "        return corpus\n",
    "\n",
    "    def fit(self, window_size=15, vector_size=300, min_count=10, workers=8, epochs=10):\n",
    "        # Initialize the Word2Vec model with gensim\n",
    "        print(\"Initializing Word2Vec model...\")\n",
    "        self.model = word2vec.Word2Vec(sentences=[self.corpus], vector_size=vector_size, window=window_size, min_count=min_count, workers=workers, epochs=epochs)\n",
    "        print(\"Model training completed.\")\n",
    "\n",
    "    def save(self, model_path):\n",
    "        # Save the model\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        # Load the model\n",
    "        self.model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        # Lowercase the corpus\n",
    "        print(\"Lowercasing...\")\n",
    "        corpus = corpus.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        print(\"Removing special characters...\")\n",
    "        corpus = re.sub(r'[^a-záàéèíìóòúùñüç\\s]', ' ', corpus)\n",
    "        \n",
    "        # Tokenize the corpus\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus = nltk.word_tokenize(corpus)\n",
    "        \n",
    "        # Eliminate last token (probably incomplete word)\n",
    "        corpus = corpus[:-1]\n",
    "        \n",
    "        return corpus\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        # Get the embedding of a word\n",
    "        try:\n",
    "            return self.model.wv[word]\n",
    "        except KeyError:\n",
    "            print(f\"Word '{word}' not in vocabulary.\")\n",
    "            return None\n",
    "\n",
    "    def print_vocab(self):\n",
    "        print(\"Vocabulary:\", list(self.model.wv.index_to_key))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model amb 100MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.save('models/word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.print_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.model.wv.most_similar(\"negre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model amb 500MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_500.model.wv.most_similar(\"inshalla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model amb 1GB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_1024 = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model.get_embedding(\"hola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model amb totes les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec_model_full_data = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importació_data import read_all_ts_data, reformat_data, create_corpus, preprocess, flattened_corpus_count\n",
    "from importació_data import pair_list_to_x_y\n",
    "from model_bàsic import build_and_compile_model_better\n",
    "import tensorflow as tf\n",
    "from model_bàsic import compute_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definim la funció de preprocessament de text tenint en compte les stopwords del català.\n",
    "- Llegim totes les dades de text similarity dividint-les en train, test i val. \n",
    "- Reformatejem les dades per a que siguin l'estructura List[Tuple[str, str, float]]. \n",
    "- Definim el corpus i el diccionari amb totes les paraules.\n",
    "- Creem un diccionari de python amb tots els indexs com a claus i amb la repetició de les paraules com a valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importació_data import stopwords_cat\n",
    "stpw_cat = stopwords_cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = lambda x: preprocess(x, stpw_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = read_all_ts_data()\n",
    "train, test, val = reformat_data(train, test, val)\n",
    "corpus, all_words = create_corpus(train, test, val, preprocess=prepro)\n",
    "flat_corpus = flattened_corpus_count(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compartació amb diferents models de Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onehot import map_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un embedding OneHot té tamany igual a la llargada del diccionari. En la importació de dades ja hem eliminat les stopwords per reduïr la dimensió, però ara també eliminarem del embedding aquelles paraules que es repeteixen masses poques vegades o massa sovint. Per aconseguir això creem una llista que conté els indexs de les paraules que sí que utilitzarem i la passem com a argument a la funció *map_one_hot()*, per reduïr la dimensió del embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate from all_words, the words that are repeated very few times or too much times\n",
    "keys_preprocess = [index  for index in all_words if flat_corpus[index] > 10 and flat_corpus[index] < 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertim les paraules a vectors OneHot amb la funció *map_one_hot()*. Aquesta funció crea un vector de zeros de la mida del diccionari i posa un 1 a la posició de la paraula en el diccionari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_one_hot_train = map_one_hot(train, all_words, keys_preprocess)\n",
    "mapped_one_hot_test = map_one_hot(test, all_words, keys_preprocess)\n",
    "mapped_one_hot_val = map_one_hot(val, all_words, keys_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separem el X i Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_oh, y_train_oh = pair_list_to_x_y(mapped_one_hot_train)\n",
    "x_val_oh, y_val_oh = pair_list_to_x_y(mapped_one_hot_val)\n",
    "x_test_oh, y_test_oh = pair_list_to_x_y(mapped_one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_oh: int = 64\n",
    "num_epochs_oh: int = 64\n",
    "\n",
    "train_dataset_oh = tf.data.Dataset.from_tensor_slices((x_train_oh, y_train_oh))\n",
    "train_dataset_oh = train_dataset_oh.shuffle(buffer_size=len(x_train_oh)).batch(batch_size_oh)\n",
    "\n",
    "val_dataset_oh = tf.data.Dataset.from_tensor_slices((x_val_oh, y_val_oh))\n",
    "val_dataset_oh = val_dataset_oh.batch(batch_size_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_oh = len(keys_preprocess)\n",
    "model_oh = build_and_compile_model_better(embedding_size = embedding_size_oh)\n",
    "tf.keras.utils.plot_model(model_oh, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_oh.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_oh.fit(train_dataset_oh, epochs=num_epochs_oh, validation_data=val_dataset_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_oh, x_train_oh, y_train_oh)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_oh, x_val_oh, y_val_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_oh, x_test_oh, y_test_oh)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec preentrenats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_tf_idf import map_pairs_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_FILE = \"\"\n",
    "\n",
    "\n",
    "USE_MMAP = False\n",
    "if USE_MMAP:\n",
    "    from gensim.models.fasttext import FastTextKeyedVectors\n",
    "    MMAP_PATH = 'cc.es.gensim.bin'\n",
    "    # wv_model.save(MMAP_PATH)\n",
    "    wv_model = FastTextKeyedVectors.load(MMAP_PATH, mmap='r')\n",
    "else:\n",
    "    from gensim.models import fasttext\n",
    "    wv_model = fasttext.load_facebook_vectors(WORD_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Mitjana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_mean_train = map_pairs_w2v(train, wv_model, dictionary=all_words, preprocess=prepro)\n",
    "mapped_w2v_mean_test = map_pairs_w2v(test, wv_model, dictionary=all_words, preprocess=prepro)\n",
    "mapped_w2v_mean_val = map_pairs_w2v(val, wv_model, dictionary=all_words, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_mean, y_train_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_train)\n",
    "x_val_w2v_mean, y_val_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_val)\n",
    "x_test_w2v_mean, y_test_w2v_mean = pair_list_to_x_y(mapped_w2v_mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_mean: int = 64\n",
    "num_epochs_w2v_mean: int = 64\n",
    "\n",
    "train_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_train_w2v_mean, y_train_w2v_mean))\n",
    "train_dataset_w2v_mean = train_dataset_w2v_mean.shuffle(buffer_size=len(x_train_w2v_mean)).batch(batch_size_w2v_mean)\n",
    "\n",
    "val_dataset_w2v_mean = tf.data.Dataset.from_tensor_slices((x_val_w2v_mean, y_val_w2v_mean))\n",
    "val_dataset_w2v_mean = val_dataset_w2v_mean.batch(batch_size_w2v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_w2v_mean = 300\n",
    "model_w2v_mean = build_and_compile_model_better(embedding_size = embedding_size_w2v_mean)\n",
    "tf.keras.utils.plot_model(model_w2v_mean, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_mean.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_mean.fit(train_dataset_w2v_mean, epochs=num_epochs_w2v_mean, validation_data=val_dataset_w2v_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_mean, x_train_w2v_mean, y_train_w2v_mean)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_mean, x_val_w2v_mean, y_val_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_mean, x_test_w2v_mean, y_test_w2v_mean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "modelo_tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_w2v_tfidf_train = map_pairs_w2v(train, wv_model, dictionary=all_words,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_test = map_pairs_w2v(test, wv_model, dictionary=all_words,tf_idf_model=modelo_tfidf, preprocess=prepro)\n",
    "mapped_w2v_tfidf_val = map_pairs_w2v(val, wv_model, dictionary=all_words,tf_idf_model=modelo_tfidf, preprocess=prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v_tfidf, y_train_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_train)\n",
    "x_val_w2v_tfidf, y_val_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_val)\n",
    "x_test_w2v_tfidf, y_test_w2v_tfidf = pair_list_to_x_y(mapped_w2v_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_w2v_tfidf: int = 64\n",
    "num_epochs_w2v_tfidf: int = 64\n",
    "\n",
    "train_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_train_w2v_tfidf, y_train_w2v_tfidf))\n",
    "train_dataset_w2v_tfidf = train_dataset_w2v_tfidf.shuffle(buffer_size=len(x_train_w2v_tfidf)).batch(batch_size_w2v_tfidf)\n",
    "\n",
    "val_dataset_w2v_tfidf = tf.data.Dataset.from_tensor_slices((x_val_w2v_tfidf, y_val_w2v_tfidf))\n",
    "val_dataset_w2v_tfidf = val_dataset_w2v_tfidf.batch(batch_size_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_w2v_tfidf = 300\n",
    "model_w2v_tfidf = build_and_compile_model_better(embedding_size = embedding_size_w2v_tfidf)\n",
    "tf.keras.utils.plot_model(model_w2v_tfidf, show_shapes=True, show_layer_activations=True)\n",
    "print(model_w2v_tfidf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_tfidf.fit(train_dataset_w2v_tfidf, epochs=num_epochs_w2v_tfidf, validation_data=val_dataset_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb la partició de validació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_w2v_tfidf, x_train_w2v_tfidf, y_train_w2v_tfidf)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_w2v_tfidf, x_val_w2v_tfidf, y_val_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (test): {compute_pearson(model_w2v_tfidf, x_test_w2v_tfidf, y_test_w2v_tfidf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy_embed import map_spacy_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"ca_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passem les particions de train, test i val a vectors amb la funció *map_spacy()*. Aquesta funció utilitza el model de SpaCy per a convertir les paraules a vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_spacy_train = map_spacy_embed(train, spacy_model)\n",
    "mapped_spacy_test = map_spacy_embed(test, spacy_model)\n",
    "mapped_spacy_val = map_spacy_embed(val, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividim les particions en X i Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_sp, y_train_sp = pair_list_to_x_y(mapped_spacy_train)\n",
    "x_val_sp, y_val_sp = pair_list_to_x_y(mapped_spacy_val)\n",
    "x_test_sp, y_test_sp = pair_list_to_x_y(mapped_spacy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenem el model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_sp: int = 64\n",
    "num_epochs_sp: int = 64\n",
    "\n",
    "train_dataset_sp = tf.data.Dataset.from_tensor_slices((x_train_sp, y_train_sp))\n",
    "train_dataset_sp = train_dataset_sp.shuffle(buffer_size=len(x_train_sp)).batch(batch_size_sp)\n",
    "\n",
    "val_dataset_sp = tf.data.Dataset.from_tensor_slices((x_val_sp, y_val_sp))\n",
    "val_dataset_sp = val_dataset_sp.batch(batch_size_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_sp = 300\n",
    "model_sp = build_and_compile_model_better(embedding_size = embedding_size_sp)\n",
    "tf.keras.utils.plot_model(model_sp, show_shapes=True, show_layer_activations=True, )\n",
    "print(model_sp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp.fit(train_dataset_sp, epochs=num_epochs_sp, validation_data=val_dataset_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluem el model amb al partició de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_train_sp, y_train_sp)}\")\n",
    "print(f\"Correlación de Pearson (validation): {compute_pearson(model_sp, x_val_sp, y_val_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provem el model amb la partició de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlación de Pearson (train): {compute_pearson(model_sp, x_test_sp, y_test_sp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ca_core_news_trf\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep\n",
    "\n",
    "sentence_pairs = [(\"El llibre va caure per la finestra.\", \"El llibre va sortir volant.\"),\n",
    "                  (\"M'agrades.\", \"T'estimo.\"),\n",
    "                  (\"M'agrada el sol i la calor\", \"A la Garrotxa plou molt.\")]\n",
    "\n",
    "predictions = pipe(prepare(sentence_pairs), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions:\n",
    "    prediction['score'] = logit(prediction['score'])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordEmbeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
