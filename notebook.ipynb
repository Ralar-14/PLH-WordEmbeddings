{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLH - Rubén Álvarez Aragonés i Pol Pérez Prades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from -r requirements.txt (line 1)) (2.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: spacy in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.4)\n",
      "Requirement already satisfied: scipy==1.10.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from -r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from -r requirements.txt (line 5)) (4.3.2)\n",
      "Collecting torch (from -r requirements.txt (line 6))\n",
      "  Downloading torch-2.3.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow->-r requirements.txt (line 1)) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.31.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (4.66.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from spacy->-r requirements.txt (line 3)) (3.4.0)\n",
      "Collecting filelock (from torch->-r requirements.txt (line 6))\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch->-r requirements.txt (line 6))\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 6))\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 6))\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch->-r requirements.txt (line 6))\n",
      "  Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.2.0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 6))\n",
      "  Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 6))\n",
      "  Downloading tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 3)) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 3)) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from jinja2->spacy->-r requirements.txt (line 3)) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->-r requirements.txt (line 6))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ralva\\desktop\\university\\4t-cuatrimestre-gia\\plh\\plh-wordembeddings\\wordembeddings\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow->-r requirements.txt (line 1)) (0.1.2)\n",
      "Downloading torch-2.3.0-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "   ---------------------------------------- 0.0/159.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.7/159.8 MB 21.1 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 1.1/159.8 MB 17.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.8/159.8 MB 46.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 7.9/159.8 MB 45.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 12.6/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 16.5/159.8 MB 73.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 18.9/159.8 MB 93.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 18.9/159.8 MB 93.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 19.5/159.8 MB 50.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 23.4/159.8 MB 43.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 26.0/159.8 MB 40.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 29.0/159.8 MB 38.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 31.6/159.8 MB 59.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 35.1/159.8 MB 65.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 38.8/159.8 MB 72.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 42.1/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 46.1/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 49.4/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 53.2/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 56.7/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 60.2/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 63.9/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 67.6/159.8 MB 73.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 71.2/159.8 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 74.8/159.8 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 78.5/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 82.0/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 85.6/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 89.0/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 92.4/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 96.0/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 99.7/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------- ------------- 103.5/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 106.9/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 109.8/159.8 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 113.4/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 116.5/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 119.5/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 122.5/159.8 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 125.8/159.8 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 129.3/159.8 MB 73.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 132.3/159.8 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 135.5/159.8 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 139.1/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 142.6/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 145.2/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 149.1/159.8 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 153.0/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  156.7/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  159.8/159.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 159.8/159.8 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   ---------------------------------------- 0.0/228.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.9/228.5 MB 82.6 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 7.9/228.5 MB 84.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 11.7/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 16.1/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 20.2/228.5 MB 93.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 23.8/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 27.2/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 30.9/228.5 MB 72.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 34.9/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 39.0/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 42.7/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 46.4/228.5 MB 81.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 51.4/228.5 MB 93.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 55.2/228.5 MB 93.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 59.8/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 63.9/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 67.4/228.5 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 70.1/228.5 MB 73.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 72.3/228.5 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 75.6/228.5 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 79.3/228.5 MB 65.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 84.0/228.5 MB 81.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 88.0/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 92.6/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 96.9/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------------- --------------------- 100.5/228.5 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------------- --------------------- 105.4/228.5 MB 93.9 MB/s eta 0:00:02\n",
      "   ------------------ -------------------- 109.7/228.5 MB 93.9 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 114.7/228.5 MB 93.9 MB/s eta 0:00:02\n",
      "   ------------------- ------------------ 119.6/228.5 MB 108.8 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 124.1/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 128.1/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 132.7/228.5 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 136.3/228.5 MB 93.0 MB/s eta 0:00:01\n",
      "   ----------------------- --------------- 139.4/228.5 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 141.5/228.5 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 142.2/228.5 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 143.8/228.5 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 146.2/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 148.6/228.5 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 152.1/228.5 MB 46.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 155.8/228.5 MB 65.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 159.5/228.5 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 163.2/228.5 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 164.1/228.5 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 166.6/228.5 MB 54.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 168.8/228.5 MB 54.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 172.1/228.5 MB 46.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 175.9/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 180.4/228.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 184.9/228.5 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------- ------ 190.8/228.5 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ----- 195.3/228.5 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 196.0/228.5 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 200.3/228.5 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 204.8/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 208.7/228.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 213.2/228.5 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 217.9/228.5 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 221.5/228.5 MB 93.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  225.4/228.5 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  227.4/228.5 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  228.5/228.5 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 228.5/228.5 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------  3.5/3.5 MB 113.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 MB 113.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 32.2 MB/s eta 0:00:00\n",
      "Downloading tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.4 kB ? eta -:--:--\n",
      "   ------------------------------------- - 276.5/286.4 kB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 286.4/286.4 kB 5.9 MB/s eta 0:00:00\n",
      "Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "   ---------------------------------------- 0.0/316.1 kB ? eta -:--:--\n",
      "   -------------------------------------- - 307.2/316.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 316.1/316.1 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 105.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 26.9 MB/s eta 0:00:00\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, sympy, networkx, mkl, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.14.0 fsspec-2024.5.0 intel-openmp-2021.4.0 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.12 tbb-2021.12.0 torch-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ralva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ralva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.ca.examples import sentences \n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class Word2VecEmbedder:\n",
    "    def __init__(self, corpus_path, corpus_size, load_model=False, model_path=None):\n",
    "        if not load_model:\n",
    "            self.corpus_path = corpus_path\n",
    "            self.corpus_size = int(corpus_size * 2**30)  # Convert GB to bytes\n",
    "            self.corpus = self.get_corpus(corpus_path)\n",
    "            self.build_vocab()\n",
    "            self.fit()\n",
    "        else:\n",
    "            try:\n",
    "                self.load(model_path)\n",
    "            except:\n",
    "                print(\"Model not found. Please check the path.\")\n",
    "                return\n",
    "\n",
    "    def get_corpus(self, corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            corpus = f.read(self.corpus_size)\n",
    "            corpus = self.preprocess(corpus)  # Preprocess the corpus and tokenize it\n",
    "        return corpus\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        corpus = corpus.lower()\n",
    "        corpus = re.sub(r'[^a-záéíóúñü\\s]', '', corpus)\n",
    "        corpus = nltk.word_tokenize(corpus)\n",
    "        corpus = corpus[:-1]\n",
    "        return corpus\n",
    "\n",
    "    def build_vocab(self):\n",
    "        self.word_counts = Counter(self.corpus)\n",
    "        self.vocab = {word: i for i, word in enumerate(self.word_counts.keys())}\n",
    "        self.inv_vocab = {i: word for word, i in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def generate_training_data(self, window_size):\n",
    "        training_data = []\n",
    "        for i, word in enumerate(self.corpus):\n",
    "            target_word = self.vocab[word]\n",
    "            context_words = [self.vocab[self.corpus[i+j]] for j in range(-window_size, window_size+1) if j != 0 and 0 <= i+j < len(self.corpus)]\n",
    "            for context_word in context_words:\n",
    "                training_data.append((target_word, context_word))\n",
    "        return training_data\n",
    "\n",
    "    def fit(self, window_size=5, vector_size=100, epochs=10, lr=0.01):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = Word2VecModel(self.vocab_size, vector_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        training_data = self.generate_training_data(window_size)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for target, context in training_data:\n",
    "                target = torch.tensor([target], dtype=torch.long).to(self.device)\n",
    "                context = torch.tensor([context], dtype=torch.long).to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(target)\n",
    "                loss = self.loss_fn(output, context)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(training_data)}')\n",
    "\n",
    "    def save(self, model_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.model = Word2VecModel(self.vocab_size, vector_size).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        word_idx = self.vocab.get(word)\n",
    "        if word_idx is not None:\n",
    "            return self.model.input_embeddings.weight[word_idx].cpu().detach().numpy()\n",
    "        else:\n",
    "            print(\"Word not in vocabulary.\")\n",
    "            return None\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, vector_size):\n",
    "        super(Word2VecModel, self).__init__()\n",
    "        self.input_embeddings = nn.Embedding(vocab_size, vector_size)\n",
    "        self.output_embeddings = nn.Linear(vector_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.input_embeddings(inputs)\n",
    "        output = self.output_embeddings(embeds)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedder:\n",
    "    def __init__(self, corpus_path, corpus_size, load_model=False, model_path=None):\n",
    "        if not load_model:\n",
    "            self.corpus_path = corpus_path\n",
    "            self.corpus_size = int(corpus_size * 2**30)  # Convert GB to bytes\n",
    "            self.corpus = self.get_corpus(corpus_path)\n",
    "            self.fit()\n",
    "        else:\n",
    "            try:\n",
    "                self.load(model_path)\n",
    "            except:\n",
    "                print(\"Model not found. Please check the path.\")\n",
    "                return\n",
    "\n",
    "    def get_corpus(self, corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            print(\"Reading corpus...\")\n",
    "            corpus = f.read(self.corpus_size)\n",
    "            print(\"Preprocessing corpus...\")\n",
    "            corpus = self.preprocess(corpus)  # Preprocess the corpus and tokenize it\n",
    "        return corpus\n",
    "\n",
    "    def fit(self, window_size=5, vector_size=100, min_count=10, workers=8, epochs=10):\n",
    "        # Initialize the Word2Vec model with gensim\n",
    "        print(\"Initializing Word2Vec model...\")\n",
    "        self.model = Word2Vec(vector_size=vector_size, window=window_size, min_count=min_count, workers=workers)\n",
    "        \n",
    "        # Build vocabulary from the corpus\n",
    "        print(\"Building vocabulary...\")\n",
    "        self.model.build_vocab(self.corpus)\n",
    "        \n",
    "        # Train the model on the corpus, using GPU if available\n",
    "        print(\"Training model...\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(device)\n",
    "        self.model.train(self.corpus, total_examples=self.model.corpus_count, epochs=epochs, compute_loss=True)\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.wv.vectors = torch.tensor(self.model.wv.vectors, device=device)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        # Save the model\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        # Load the model\n",
    "        self.model = Word2Vec.load(model_path)\n",
    "\n",
    "    def preprocess(self, corpus):\n",
    "        # Lowercase the corpus\n",
    "        print(\"Lowercasing...\")\n",
    "        corpus = corpus.lower()\n",
    "        \n",
    "        # Remove special characters\n",
    "        print(\"Removing special characters...\")\n",
    "        corpus = re.sub(r'[^a-záéíóúñü\\s]', '', corpus)\n",
    "        \n",
    "        # Tokenize the corpus\n",
    "        print(\"Tokenizing...\")\n",
    "        corpus = nltk.word_tokenize(corpus)\n",
    "        \n",
    "        # Eliminate last token (probably incomplete word)\n",
    "        corpus = corpus[:-1]\n",
    "        \n",
    "        return corpus\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        # Get the embedding of a word\n",
    "        return self.model.wv[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model amb 100MB de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2VecEmbedder('corpus\\catalan_general_crawling.txt', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reduu',\n",
       " 'els',\n",
       " 'costos',\n",
       " 'dels',\n",
       " 'processos',\n",
       " 'administratius',\n",
       " 'al',\n",
       " 'vostre',\n",
       " 'organisme',\n",
       " 'públic',\n",
       " 'eviteu',\n",
       " 'els',\n",
       " 'desplaaments',\n",
       " 'i',\n",
       " 'prdua',\n",
       " 'de',\n",
       " 'temps',\n",
       " 'als',\n",
       " 'ciutadans',\n",
       " 'en',\n",
       " 'les',\n",
       " 'seves',\n",
       " 'gestions',\n",
       " 'oferiu',\n",
       " 'una',\n",
       " 'administració',\n",
       " 'més',\n",
       " 'transparent',\n",
       " 'a',\n",
       " 'ciutadans',\n",
       " 'i',\n",
       " 'empreses',\n",
       " 'ens',\n",
       " 'grans',\n",
       " 'i',\n",
       " 'petits',\n",
       " 'experimenten',\n",
       " 'aquesta',\n",
       " 'transformació',\n",
       " 'amb',\n",
       " 'xit',\n",
       " 'grcies',\n",
       " 'al',\n",
       " 'suport',\n",
       " 'de',\n",
       " 'laoc',\n",
       " 'departament',\n",
       " 'de',\n",
       " 'sistemes',\n",
       " 'dinformació',\n",
       " 'i',\n",
       " 'processos',\n",
       " 'via',\n",
       " 'oberta',\n",
       " 'ens',\n",
       " 'ha',\n",
       " 'perms',\n",
       " 'fer',\n",
       " 'efectiu',\n",
       " 'el',\n",
       " 'dret',\n",
       " 'dels',\n",
       " 'ciutadans',\n",
       " 'a',\n",
       " 'no',\n",
       " 'aportar',\n",
       " 'documents',\n",
       " 'eliminant',\n",
       " 'paper',\n",
       " 'i',\n",
       " 'simplificant',\n",
       " 'procediments',\n",
       " 'efact',\n",
       " 'proporciona',\n",
       " 'informació',\n",
       " 'indispensable',\n",
       " 'per',\n",
       " 'a',\n",
       " 'la',\n",
       " 'realització',\n",
       " 'de',\n",
       " 'les',\n",
       " 'auditories',\n",
       " 'del',\n",
       " 'registre',\n",
       " 'comptable',\n",
       " 'de',\n",
       " 'factures',\n",
       " 'de',\n",
       " 'les',\n",
       " 'administracions',\n",
       " 'públiques',\n",
       " 'catalanes',\n",
       " 'coordinador',\n",
       " 'del',\n",
       " 'departament',\n",
       " 'dinformtica',\n",
       " 'el',\n",
       " 'servei',\n",
       " 'via',\n",
       " 'oberta',\n",
       " 'és',\n",
       " 'el',\n",
       " 'que',\n",
       " 'ha',\n",
       " 'aportat',\n",
       " 'majors',\n",
       " 'avantatges',\n",
       " 'per',\n",
       " 'als',\n",
       " 'ciutadans',\n",
       " 'amb',\n",
       " 'l',\n",
       " 'enotum',\n",
       " 'hem',\n",
       " 'escurat',\n",
       " 'els',\n",
       " 'procediments',\n",
       " 'en',\n",
       " 'dies',\n",
       " 'quasi',\n",
       " 'un',\n",
       " 'menys',\n",
       " 'coordinadora',\n",
       " 'dorganització',\n",
       " 'de',\n",
       " 'persones',\n",
       " 'i',\n",
       " 'eadministració',\n",
       " 'via',\n",
       " 'oberta',\n",
       " 'ofereix',\n",
       " 'millores',\n",
       " 'per',\n",
       " 'als',\n",
       " 'ciutadans',\n",
       " 'al',\n",
       " 'no',\n",
       " 'haver',\n",
       " 'daportar',\n",
       " 'cap',\n",
       " 'document',\n",
       " 'responsable',\n",
       " 'dinformtica',\n",
       " 'i',\n",
       " 'administració',\n",
       " 'electrnica',\n",
       " 'etram',\n",
       " 'ens',\n",
       " 'ha',\n",
       " 'perms',\n",
       " 'implantar',\n",
       " 'un',\n",
       " 'servei',\n",
       " 'de',\n",
       " 'tramitació',\n",
       " 'electrnica',\n",
       " 'per',\n",
       " 'als',\n",
       " 'ciutadans',\n",
       " 'de',\n",
       " 'forma',\n",
       " 'rpida',\n",
       " 'senzilla',\n",
       " 'i',\n",
       " 'amb',\n",
       " 'un',\n",
       " 'cost',\n",
       " 'redut',\n",
       " 'els',\n",
       " 'municipis',\n",
       " 'amb',\n",
       " 'pocs',\n",
       " 'habitants',\n",
       " 'trobem',\n",
       " 'en',\n",
       " 'els',\n",
       " 'serveis',\n",
       " 'de',\n",
       " 'laoc',\n",
       " 'la',\n",
       " 'gratutat',\n",
       " 'i',\n",
       " 'la',\n",
       " 'comoditat',\n",
       " 'necessries',\n",
       " 'per',\n",
       " 'dur',\n",
       " 'a',\n",
       " 'terme',\n",
       " 'el',\n",
       " 'nostre',\n",
       " 'dia',\n",
       " 'a',\n",
       " 'dia',\n",
       " 'les',\n",
       " 'tcat',\n",
       " 'han',\n",
       " 'perms',\n",
       " 'incorporar',\n",
       " 'de',\n",
       " 'forma',\n",
       " 'segura',\n",
       " 'la',\n",
       " 'signatura',\n",
       " 'electrnica',\n",
       " 'dins',\n",
       " 'dels',\n",
       " 'nostres',\n",
       " 'procediments',\n",
       " 'afavorint',\n",
       " 'la',\n",
       " 'transformació',\n",
       " 'digital',\n",
       " 'de',\n",
       " 'la',\n",
       " 'nostra',\n",
       " 'activitat',\n",
       " 'cap',\n",
       " 'de',\n",
       " 'departament',\n",
       " 'de',\n",
       " 'sistemes',\n",
       " 'i',\n",
       " 'tecnologies',\n",
       " 'de',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'amb',\n",
       " 'el',\n",
       " 'desplegament',\n",
       " 'de',\n",
       " 'l',\n",
       " 'idcat',\n",
       " 'hem',\n",
       " 'apropat',\n",
       " 'lajuntament',\n",
       " 'a',\n",
       " 'la',\n",
       " 'ciutadania',\n",
       " 'mitjanant',\n",
       " 'els',\n",
       " 'serveis',\n",
       " 'de',\n",
       " 'govern',\n",
       " 'obert',\n",
       " 'de',\n",
       " 'laoc',\n",
       " 'hem',\n",
       " 'pogut',\n",
       " 'fer',\n",
       " 'fcil',\n",
       " 'el',\n",
       " 'que',\n",
       " 'sembla',\n",
       " 'difícil',\n",
       " 'al',\n",
       " 'tauler',\n",
       " 'electrnic',\n",
       " 'pots',\n",
       " 'penjar',\n",
       " 'fins',\n",
       " 'i',\n",
       " 'tot',\n",
       " 'el',\n",
       " 'projecte',\n",
       " 'sencer',\n",
       " 'i',\n",
       " 'al',\n",
       " 'final',\n",
       " 'et',\n",
       " 'permet',\n",
       " 'fer',\n",
       " 'també',\n",
       " 'la',\n",
       " 'diligncia',\n",
       " 'rea',\n",
       " 'de',\n",
       " 'promoció',\n",
       " 'econmica',\n",
       " 'administració',\n",
       " 'i',\n",
       " 'hisenda',\n",
       " 'el',\n",
       " 'sobre',\n",
       " 'digital',\n",
       " 'i',\n",
       " 'la',\n",
       " 'pscp',\n",
       " 'han',\n",
       " 'aconseguit',\n",
       " 'una',\n",
       " 'comunió',\n",
       " 'senzilla',\n",
       " 'entre',\n",
       " 'empreses',\n",
       " 'i',\n",
       " 'administració',\n",
       " 'per',\n",
       " 'universalitzar',\n",
       " 'la',\n",
       " 'compra',\n",
       " 'pública',\n",
       " 'electrnica',\n",
       " 'l',\n",
       " 'eset',\n",
       " 'és',\n",
       " 'la',\n",
       " 'implantació',\n",
       " 'dun',\n",
       " 'nou',\n",
       " 'sistema',\n",
       " 'de',\n",
       " 'treball',\n",
       " 'que',\n",
       " 'facilita',\n",
       " 'la',\n",
       " 'feina',\n",
       " 'del',\n",
       " 'dia',\n",
       " 'a',\n",
       " 'dia',\n",
       " 'cap',\n",
       " 'del',\n",
       " 'servei',\n",
       " 'de',\n",
       " 'contractació',\n",
       " 'i',\n",
       " 'compres',\n",
       " 'el',\n",
       " 'sobre',\n",
       " 'digital',\n",
       " 'una',\n",
       " 'experincia',\n",
       " 'imprescindible',\n",
       " 'per',\n",
       " 'a',\n",
       " 'la',\n",
       " 'bona',\n",
       " 'administració',\n",
       " 'amb',\n",
       " 'estalvi',\n",
       " 'de',\n",
       " 'recursos',\n",
       " 'i',\n",
       " 'millora',\n",
       " 'de',\n",
       " 'la',\n",
       " 'seguretat',\n",
       " 'jurídica',\n",
       " 'i',\n",
       " 'la',\n",
       " 'transparncia',\n",
       " 'rea',\n",
       " 'dorganització',\n",
       " 'i',\n",
       " 'administració',\n",
       " 'electrnica',\n",
       " 'el',\n",
       " 'desplegament',\n",
       " 'de',\n",
       " 'la',\n",
       " 'valisa',\n",
       " 'electrnica',\n",
       " 'ha',\n",
       " 'estat',\n",
       " 'clau',\n",
       " 'en',\n",
       " 'el',\n",
       " 'procés',\n",
       " 'de',\n",
       " 'transformació',\n",
       " 'digital',\n",
       " 'dels',\n",
       " 'nostres',\n",
       " 'procediments',\n",
       " 'interns',\n",
       " 'l',\n",
       " 'hstia',\n",
       " 'permet',\n",
       " 'el',\n",
       " 'treball',\n",
       " 'en',\n",
       " 'temps',\n",
       " 'real',\n",
       " 'i',\n",
       " 'des',\n",
       " 'de',\n",
       " 'qualsevol',\n",
       " 'lloc',\n",
       " 'així',\n",
       " 'com',\n",
       " 'sistematitzar',\n",
       " 'la',\n",
       " 'prctica',\n",
       " 'professional',\n",
       " 'recollir',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'ordenadament',\n",
       " 'i',\n",
       " 'amb',\n",
       " 'el',\n",
       " 'mateix',\n",
       " 'llenguatge',\n",
       " 'consulta',\n",
       " 'els',\n",
       " 'materials',\n",
       " 'del',\n",
       " 'congrés',\n",
       " 'de',\n",
       " 'govern',\n",
       " 'digital',\n",
       " 'governs',\n",
       " 'transparents',\n",
       " 'fluids',\n",
       " 'dinmics',\n",
       " 'líquids',\n",
       " 'un',\n",
       " 'bon',\n",
       " 'lema',\n",
       " 'pel',\n",
       " 'principal',\n",
       " 'objectiu',\n",
       " 'de',\n",
       " 'la',\n",
       " 'governana',\n",
       " 'del',\n",
       " 'segle',\n",
       " 'xxi',\n",
       " 'democratitzarho',\n",
       " 'tot',\n",
       " 'confluncies',\n",
       " 'rius',\n",
       " 'cooperació',\n",
       " 'catalunya',\n",
       " 'mediterrnia',\n",
       " 'mar',\n",
       " 'de',\n",
       " 'drets',\n",
       " 'a',\n",
       " 'favor',\n",
       " 'totes',\n",
       " 'les',\n",
       " 'administracions',\n",
       " 'moventse',\n",
       " 'per',\n",
       " 'posarse',\n",
       " 'al',\n",
       " 'dia',\n",
       " 'i',\n",
       " 'millorar',\n",
       " 'tot',\n",
       " 'aprofitant',\n",
       " 'la',\n",
       " 'revolució',\n",
       " 'digital',\n",
       " 'en',\n",
       " 'contra',\n",
       " 'quants',\n",
       " 'cops',\n",
       " 'estem',\n",
       " 'reinventant',\n",
       " 'la',\n",
       " 'roda',\n",
       " 'i',\n",
       " 'quantes',\n",
       " 'quantes',\n",
       " 'oportunitats',\n",
       " 'perdudes',\n",
       " 'de',\n",
       " 'ferho',\n",
       " 'una',\n",
       " 'única',\n",
       " 'vegada',\n",
       " 'i',\n",
       " 'de',\n",
       " 'forma',\n",
       " 'coordinada',\n",
       " 'i',\n",
       " 'collaborativa',\n",
       " 'la',\n",
       " 'transparncia',\n",
       " 'és',\n",
       " 'una',\n",
       " 'oportunitat',\n",
       " 'hem',\n",
       " 'de',\n",
       " 'perdre',\n",
       " 'tota',\n",
       " 'por',\n",
       " 'a',\n",
       " 'explicar',\n",
       " 'qu',\n",
       " 'fem',\n",
       " 'la',\n",
       " 'conclusió',\n",
       " 'de',\n",
       " 'la',\n",
       " 'taula',\n",
       " 'dalcaldies',\n",
       " 'de',\n",
       " 'la',\n",
       " 'jornada',\n",
       " 'de',\n",
       " 'govern',\n",
       " 'obert',\n",
       " 'pictwittercomerbglsixzm',\n",
       " 'el',\n",
       " 'director',\n",
       " 'general',\n",
       " 'de',\n",
       " 'participació',\n",
       " 'ciutadana',\n",
       " 'ens',\n",
       " 'convida',\n",
       " 'a',\n",
       " 'transformar',\n",
       " 'les',\n",
       " 'administracions',\n",
       " 'públiques',\n",
       " 'a',\n",
       " 'partir',\n",
       " 'de',\n",
       " 'la',\n",
       " 'participació',\n",
       " 'ciutadana',\n",
       " 'ens',\n",
       " 'cal',\n",
       " 'que',\n",
       " 'all',\n",
       " 'que',\n",
       " 'preocupa',\n",
       " 'i',\n",
       " 'ocupa',\n",
       " 'els',\n",
       " 'governants',\n",
       " 'formi',\n",
       " 'part',\n",
       " 'dall',\n",
       " 'en',\n",
       " 'qu',\n",
       " 'participa',\n",
       " 'la',\n",
       " 'ciutadania',\n",
       " 'pictwittercomnwqrezscs',\n",
       " 'a',\n",
       " 'moltes',\n",
       " 'institucions',\n",
       " 'encara',\n",
       " 'els',\n",
       " 'sona',\n",
       " 'xinés',\n",
       " 'aix',\n",
       " 'de',\n",
       " 'les',\n",
       " 'dades',\n",
       " 'obertes',\n",
       " 'i',\n",
       " 'la',\n",
       " 'transparncia',\n",
       " 'de',\n",
       " 'que',\n",
       " 'serveix',\n",
       " 'que',\n",
       " 'hi',\n",
       " 'hagi',\n",
       " 'un',\n",
       " 'portal',\n",
       " 'si',\n",
       " 'llavors',\n",
       " 'no',\n",
       " 'hi',\n",
       " 'ha',\n",
       " 'dades',\n",
       " 'llavors',\n",
       " 'laccés',\n",
       " 'a',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'pels',\n",
       " 'periodistes',\n",
       " 'és',\n",
       " 'molt',\n",
       " 'parcial',\n",
       " 'oferim',\n",
       " 'eines',\n",
       " 'que',\n",
       " 'conjuntament',\n",
       " 'amb',\n",
       " 'la',\n",
       " 'metodologia',\n",
       " 'i',\n",
       " 'el',\n",
       " 'suport',\n",
       " 'necessari',\n",
       " 'fan',\n",
       " 'possible',\n",
       " 'lassoliment',\n",
       " 'dun',\n",
       " 'govern',\n",
       " 'digital',\n",
       " 'posem',\n",
       " 'al',\n",
       " 'vostre',\n",
       " 'abast',\n",
       " 'tot',\n",
       " 'el',\n",
       " 'coneixement',\n",
       " 'formació',\n",
       " 'guies',\n",
       " 'normatives',\n",
       " 'etc',\n",
       " 'tenim',\n",
       " 'eines',\n",
       " 'per',\n",
       " 'gestionar',\n",
       " 'gilment',\n",
       " 'part',\n",
       " 'del',\n",
       " 'procés',\n",
       " 'administratiu',\n",
       " 'del',\n",
       " 'vostre',\n",
       " 'ens',\n",
       " 'el',\n",
       " 'nostre',\n",
       " 'equip',\n",
       " 'far',\n",
       " 'tot',\n",
       " 'el',\n",
       " 'possible',\n",
       " 'per',\n",
       " 'resoldre',\n",
       " 'les',\n",
       " 'vostres',\n",
       " 'incidncies',\n",
       " 'sabem',\n",
       " 'que',\n",
       " 'es',\n",
       " 'tracta',\n",
       " 'duna',\n",
       " 'decisió',\n",
       " 'molt',\n",
       " 'important',\n",
       " 'per',\n",
       " 'al',\n",
       " 'vostre',\n",
       " 'ens',\n",
       " 'i',\n",
       " 'és',\n",
       " 'per',\n",
       " 'aix',\n",
       " 'que',\n",
       " 'us',\n",
       " 'ho',\n",
       " 'volem',\n",
       " 'posar',\n",
       " 'fcil',\n",
       " 'la',\n",
       " 'selecció',\n",
       " 'de',\n",
       " 'lactualitat',\n",
       " 'dadministració',\n",
       " 'oberta',\n",
       " 'a',\n",
       " 'la',\n",
       " 'vostra',\n",
       " 'safata',\n",
       " 'en',\n",
       " 'compliment',\n",
       " 'de',\n",
       " 'la',\n",
       " 'directiva',\n",
       " 'ce',\n",
       " 'desenvolupada',\n",
       " 'en',\n",
       " 'el',\n",
       " 'nostre',\n",
       " 'ordenament',\n",
       " 'per',\n",
       " 'larticle',\n",
       " 'de',\n",
       " 'la',\n",
       " 'llei',\n",
       " 'de',\n",
       " 'serveis',\n",
       " 'de',\n",
       " 'societat',\n",
       " 'de',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'lssi',\n",
       " 'i',\n",
       " 'seguint',\n",
       " 'les',\n",
       " 'instruccions',\n",
       " 'de',\n",
       " 'lagncia',\n",
       " 'espanyola',\n",
       " 'de',\n",
       " 'protecció',\n",
       " 'de',\n",
       " 'dades',\n",
       " 'procedim',\n",
       " 'a',\n",
       " 'informarli',\n",
       " 'detalladament',\n",
       " 'de',\n",
       " 'lús',\n",
       " 'que',\n",
       " 'es',\n",
       " 'realitza',\n",
       " 'a',\n",
       " 'la',\n",
       " 'nostra',\n",
       " 'pgina',\n",
       " 'web',\n",
       " 'aquesta',\n",
       " 'informació',\n",
       " 'no',\n",
       " 'revela',\n",
       " 'la',\n",
       " 'seva',\n",
       " 'identitat',\n",
       " 'per',\n",
       " 'sí',\n",
       " 'que',\n",
       " 'permet',\n",
       " 'la',\n",
       " 'seva',\n",
       " 'identificació',\n",
       " 'com',\n",
       " 'a',\n",
       " 'un',\n",
       " 'usuari',\n",
       " 'concret',\n",
       " 'i',\n",
       " 'pot',\n",
       " 'guardar',\n",
       " 'informació',\n",
       " 'relativa',\n",
       " 'a',\n",
       " 'la',\n",
       " 'freqüncia',\n",
       " 'amb',\n",
       " 'la',\n",
       " 'que',\n",
       " 'visita',\n",
       " 'la',\n",
       " 'pgina',\n",
       " 'web',\n",
       " 'les',\n",
       " 'seves',\n",
       " 'preferncies',\n",
       " 'de',\n",
       " 'navegació',\n",
       " 'o',\n",
       " 'aquella',\n",
       " 'informació',\n",
       " 'que',\n",
       " 'més',\n",
       " 'linteressa',\n",
       " 'el',\n",
       " 'que',\n",
       " 'ens',\n",
       " 'permet',\n",
       " 'cada',\n",
       " 'vegada',\n",
       " 'que',\n",
       " 'accedeix',\n",
       " 'a',\n",
       " 'wwwaoccat',\n",
       " 'millorar',\n",
       " 'la',\n",
       " 'qualitat',\n",
       " 'i',\n",
       " 'la',\n",
       " 'usabilitat',\n",
       " 'de',\n",
       " 'la',\n",
       " 'nostra',\n",
       " 'pgina',\n",
       " 'web',\n",
       " 'no',\n",
       " 'obstant',\n",
       " 'si',\n",
       " 'les',\n",
       " 'desactiva',\n",
       " 'pot',\n",
       " 'ser',\n",
       " 'que',\n",
       " 'la',\n",
       " 'seva',\n",
       " 'navegació',\n",
       " 'per',\n",
       " 'wwwaoccat',\n",
       " 'no',\n",
       " 'sigui',\n",
       " 'ptima',\n",
       " 'i',\n",
       " 'algunes',\n",
       " 'de',\n",
       " 'les',\n",
       " 'seves',\n",
       " 'utilitats',\n",
       " 'no',\n",
       " 'funcionin',\n",
       " 'correctament',\n",
       " 'cookies',\n",
       " 'analítiques',\n",
       " 'galetes',\n",
       " 'de',\n",
       " 'google',\n",
       " 'analytics',\n",
       " 'aquesta',\n",
       " 'pgina',\n",
       " 'web',\n",
       " 'utilitza',\n",
       " 'google',\n",
       " 'analytics',\n",
       " 'un',\n",
       " 'servei',\n",
       " 'analític',\n",
       " 'del',\n",
       " 'web',\n",
       " 'prestat',\n",
       " 'per',\n",
       " 'google',\n",
       " 'inc',\n",
       " 'una',\n",
       " 'companyia',\n",
       " 'de',\n",
       " 'delaware',\n",
       " 'loficina',\n",
       " 'principal',\n",
       " 'de',\n",
       " 'la',\n",
       " 'qual',\n",
       " 'es',\n",
       " 'troba',\n",
       " 'a',\n",
       " 'amphitheatre',\n",
       " 'parkway',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'califrnia',\n",
       " 'ca',\n",
       " 'estats',\n",
       " 'units',\n",
       " 'google',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'que',\n",
       " 'genera',\n",
       " 'la',\n",
       " 'cookie',\n",
       " 'sobre',\n",
       " 'lús',\n",
       " 'del',\n",
       " 'lloc',\n",
       " 'web',\n",
       " 'incloent',\n",
       " 'ladrea',\n",
       " 'ip',\n",
       " 'ser',\n",
       " 'directament',\n",
       " 'transmesa',\n",
       " 'i',\n",
       " 'arxivada',\n",
       " 'per',\n",
       " 'google',\n",
       " 'en',\n",
       " 'els',\n",
       " 'seus',\n",
       " 'servidors',\n",
       " 'destats',\n",
       " 'units',\n",
       " 'google',\n",
       " 'utilitzar',\n",
       " 'aquesta',\n",
       " 'informació',\n",
       " 'per',\n",
       " 'compte',\n",
       " 'nostre',\n",
       " 'amb',\n",
       " 'el',\n",
       " 'propsit',\n",
       " 'de',\n",
       " 'seguir',\n",
       " 'la',\n",
       " 'pista',\n",
       " 'del',\n",
       " 'seu',\n",
       " 'ús',\n",
       " 'del',\n",
       " 'lloc',\n",
       " 'web',\n",
       " 'google',\n",
       " 'podr',\n",
       " 'transmetre',\n",
       " 'aquesta',\n",
       " 'informació',\n",
       " 'a',\n",
       " 'tercers',\n",
       " 'quan',\n",
       " 'així',\n",
       " 'ho',\n",
       " 'requereixi',\n",
       " 'la',\n",
       " 'legislació',\n",
       " 'o',\n",
       " 'quan',\n",
       " 'aquests',\n",
       " 'tercers',\n",
       " 'processin',\n",
       " 'la',\n",
       " 'informació',\n",
       " 'per',\n",
       " 'compte',\n",
       " 'de',\n",
       " 'google',\n",
       " 'en',\n",
       " 'aquests',\n",
       " 'casos',\n",
       " 'google',\n",
       " 'no',\n",
       " 'associar',\n",
       " 'la',\n",
       " 'seva',\n",
       " 'adrea',\n",
       " 'ip',\n",
       " 'amb',\n",
       " 'cap',\n",
       " 'altra',\n",
       " 'dada',\n",
       " 'de',\n",
       " 'qu',\n",
       " 'disposi',\n",
       " 'en',\n",
       " 'utilitzar',\n",
       " 'aquesta',\n",
       " 'pgina',\n",
       " 'web',\n",
       " 'consent',\n",
       " 'el',\n",
       " 'tractament',\n",
       " 'de',\n",
       " 'la',\n",
       " 'seva',\n",
       " 'informació',\n",
       " 'per',\n",
       " 'google',\n",
       " 'en',\n",
       " 'la',\n",
       " 'forma',\n",
       " 'i',\n",
       " 'per',\n",
       " 'als',\n",
       " 'fins',\n",
       " 'anteriorment',\n",
       " 'indicats',\n",
       " 'lexercici',\n",
       " 'de',\n",
       " 'qualsevol',\n",
       " 'dret',\n",
       " 'shaur',\n",
       " 'de',\n",
       " 'realitzar',\n",
       " 'mitjanant',\n",
       " 'comunicació',\n",
       " 'directa',\n",
       " 'amb',\n",
       " 'google',\n",
       " 'per',\n",
       " 'optar',\n",
       " 'per',\n",
       " 'no',\n",
       " 'ser',\n",
       " 'rastrejats',\n",
       " 'per',\n",
       " 'google',\n",
       " 'analytics',\n",
       " 'a',\n",
       " 'través',\n",
       " 'de',\n",
       " 'tots',\n",
       " 'els',\n",
       " 'llocs',\n",
       " 'web',\n",
       " 'podeu',\n",
       " 'consultar',\n",
       " 'httptoolsgooglecomdlpagegaoptout',\n",
       " 'així',\n",
       " 'mateix',\n",
       " 'també',\n",
       " 'registra',\n",
       " 'quan',\n",
       " 'va',\n",
       " 'ser',\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model amb 500MB de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model amb 1GB de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament model de Similitud de Text Semàntic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(hidden_size: int = 64) -> tf.keras.Model:\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Concatenate(axis=-1, ),\n",
    "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = build_and_compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = baseline_model((np.ones((1, 100)), np.ones((1,100)), ), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compartació amb diferents models de Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. One Hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Word2Vec preentrenats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Word2vec permet dos formats: text i binari\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Obtenir un word-vector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ralva\\Desktop\\University\\4t-Cuatrimestre-GIA\\PLH\\PLH-WordEmbeddings\\WordEmbeddings\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ralva\\Desktop\\University\\4t-Cuatrimestre-GIA\\PLH\\PLH-WordEmbeddings\\WordEmbeddings\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32mc:\\Users\\ralva\\Desktop\\University\\4t-Cuatrimestre-GIA\\PLH\\PLH-WordEmbeddings\\WordEmbeddings\\Lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\ralva\\Desktop\\University\\4t-Cuatrimestre-GIA\\PLH\\PLH-WordEmbeddings\\WordEmbeddings\\Lib\\site-packages\\smart_open\\smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.bin'"
     ]
    }
   ],
   "source": [
    "model.save('model.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "kv = KeyedVectors.load_word2vec_format('model.bin', binary=True, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'casa'\n",
    "word2 = 'cotxe'\n",
    "\n",
    "word1_vector = kv[word1]\n",
    "word2_vector = kv[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = build_and_compile_model()\n",
    "result = w2v((word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "sentence = nlp(\"I sit on a bank.\")\n",
    "sentence[4].vector\n",
    "# -> NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RoBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ca_core_news_trf\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RoBERTa fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def prepare(sentence_pairs):\n",
    "    sentence_pairs_prep = []\n",
    "    for s1, s2 in sentence_pairs:\n",
    "        sentence_pairs_prep.append(f\"{tokenizer.cls_token} {s1}{tokenizer.sep_token}{tokenizer.sep_token} {s2}{tokenizer.sep_token}\")\n",
    "    return sentence_pairs_prep\n",
    "\n",
    "sentence_pairs = [(\"El llibre va caure per la finestra.\", \"El llibre va sortir volant.\"),\n",
    "                  (\"M'agrades.\", \"T'estimo.\"),\n",
    "                  (\"M'agrada el sol i la calor\", \"A la Garrotxa plou molt.\")]\n",
    "\n",
    "predictions = pipe(prepare(sentence_pairs), add_special_tokens=False)\n",
    "\n",
    "# convert back to scores to the original 0 and 5 interval\n",
    "for prediction in predictions:\n",
    "    prediction['score'] = logit(prediction['score'])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WordEmbeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
